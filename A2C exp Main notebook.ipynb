{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7413ea-e5d1-49c6-8dc9-977ab4dc3d63",
   "metadata": {},
   "source": [
    "# lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "905a5491-3426-4f41-9c29-9194f5398328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:121: DeprecationWarning: pkg_resources is deprecated as an API\n",
      "  warnings.warn(\"pkg_resources is deprecated as an API\", DeprecationWarning)\n",
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] =\"expandable_segments:True\"\n",
    "import gymnasium as gym\n",
    "\n",
    "#import gym\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import functools\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete, Box, Dict\n",
    "\n",
    "from pettingzoo import AECEnv\n",
    "\n",
    "\n",
    "\n",
    "import gymnasium\n",
    "\n",
    "\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "\n",
    "from gymnasium.utils import EzPickle\n",
    "\n",
    "\n",
    "\n",
    "from statistics import NormalDist\n",
    "\n",
    "import pygame\n",
    "\n",
    "from typing import Any , Generic, Iterable, Iterator, TypeVar\n",
    "ActionType = TypeVar(\"ActionType\")\n",
    "\n",
    "import collections\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20abc03-b5a9-48bc-aee4-f42c7b25ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utilities.new_models import *\n",
    "from Utilities.Transformer_risk_act_2 import *\n",
    "import utils_gym\n",
    "import env_model_class_2\n",
    "\n",
    "\n",
    "from board_env import *\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254dca59-dda5-4fbf-b5fd-6f821996a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f03415-c38b-4c21-95f9-e72d1d5295a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1729b32d-eec1-49a3-b168-a87120dce1a1",
   "metadata": {},
   "source": [
    "#  Algorithm module definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e62529f8-e3c1-45fd-9481-5dc7a2471723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_module:\n",
    "    def __init__(self, qnet_config_dict, actor_config_dict,args,device,writer,run_name,agent):\n",
    "\n",
    "        self.agent = agent\n",
    "        self.run_name =run_name \n",
    "        self.actor_config_dict = actor_config_dict\n",
    "        self.qnet_config_dict = qnet_config_dict\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.writer = writer\n",
    "        self.entropy=args.entropy\n",
    "        self.return_prob=args.return_prob\n",
    "        self.actor_wt = args.actor_wt\n",
    "        self.CE_wt = args.CE_wt\n",
    "        if self.args.small:\n",
    "            self.actor = Actor_ddqn_small(**self.actor_config_dict).to(self.device)\n",
    "            self.qf1 = QNetwork_small(**self.qnet_config_dict).to(self.device)\n",
    "            self.qf1_target = QNetwork_small(**self.qnet_config_dict).to(self.device)\n",
    "            self.target_actor = Actor_ddqn_small(**self.actor_config_dict).to(self.device)\n",
    "            \n",
    "        else:\n",
    "            self.actor = Actor_ddqn(**self.actor_config_dict).to(self.device)\n",
    "            self.qf1 = QNetwork(**self.qnet_config_dict).to(self.device)\n",
    "            self.qf1_target = QNetwork(**self.qnet_config_dict).to(self.device)\n",
    "            self.target_actor = Actor_ddqn(**self.actor_config_dict).to(self.device)\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.qf1_target.load_state_dict(self.qf1.state_dict())\n",
    "        self.q_optimizer = optim.Adam(list(self.qf1.parameters()), lr=self.args.learning_rate)\n",
    "        self.actor_optimizer = optim.Adam(list(self.actor.parameters()), lr=self.args.learning_rate)\n",
    "\n",
    "    def action_predict(self,data):\n",
    "        return self.actor(data)\n",
    "\n",
    "    def train_write(self,data,iteration,epoch):\n",
    "        #data = rb.sample(self.args.batch_size)\n",
    "        qf1_a_values, qf1_loss, actor_loss = self.train(data)\n",
    "        self.write(qf1_a_values, qf1_loss, actor_loss,epoch,iteration)\n",
    "\n",
    "    def cal_q_loss(self,qf1_a_values, next_q_value):\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        \n",
    "        return criterion(qf1_a_values, next_q_value)\n",
    "        #return nn.functional.mse_loss(qf1_a_values, next_q_value)\n",
    "\n",
    "    \n",
    "    def cal_actor_loss(self,data,entropy=False,return_prob=False):\n",
    "\n",
    "        if not entropy:\n",
    "            return -self.qf1(data.observations[:,:-1], self.actor(data.observations[:,:-1])).mean()\n",
    "        else:\n",
    "            actions,probs = self.actor(data.observations[:,:-1],return_prob=return_prob)\n",
    "            action_loss = -self.qf1(data.observations[:,:-1],actions).mean()\n",
    "\n",
    "            entropy_ = -(probs*torch.log2(probs)).sum(-1).mean()\n",
    "            return self.actor_wt*action_loss + self.CE_wt*entropy_# if not torch.isnan(cross_entropy_loss) else 0)\n",
    "        \n",
    "\n",
    "    def train(self,data):\n",
    "\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            collected_t_next = data.next_observations[:,-1]\n",
    "            next_state_actions = self.target_actor(data.next_observations[:,:-1])\n",
    "            qf1_next_target = self.qf1_target(data.next_observations[:,:-1], next_state_actions)\n",
    "\n",
    "            \n",
    "            next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * (self.args.gamma**(collected_t_next+1)).view(-1) * (qf1_next_target).view(-1)\n",
    "    \n",
    "        qf1_a_values = self.qf1(data.observations[:,:-1], data.actions).view(-1)\n",
    "        qf1_loss = self.cal_q_loss(qf1_a_values, next_q_value)\n",
    "        \n",
    "        # optimize the model\n",
    "        self.q_optimizer.zero_grad()\n",
    "        qf1_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "        \n",
    "        #if global_step % args.policy_frequency == 0:\n",
    "        actor_loss = self.cal_actor_loss(data,entropy=self.entropy,return_prob=self.return_prob)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "    \n",
    "        # update the target network\n",
    "        for param, target_param in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
    "            target_param.data.copy_(self.args.tau * param.data + (1 - self.args.tau) * target_param.data)\n",
    "        for param, target_param in zip(self.qf1.parameters(), self.qf1_target.parameters()):\n",
    "            target_param.data.copy_(self.args.tau * param.data + (1 - self.args.tau) * target_param.data)\n",
    "        return qf1_a_values, qf1_loss, actor_loss\n",
    "        \n",
    "\n",
    "    def write(self,qf1_a_values, qf1_loss, actor_loss,epoch,iteration):\n",
    "        \n",
    "        ind_epoch = epoch + (iteration-1)*self.args.update_epochs\n",
    "        self.writer.add_scalar(f\"losses/{self.agent}/qf1_values\", qf1_a_values.mean().item(), ind_epoch)\n",
    "        \n",
    "        self.writer.add_scalar(f\"losses/{self.agent}/qf1_loss\", qf1_loss.item(), ind_epoch)\n",
    "        self.writer.add_scalar(f\"losses/{self.agent}/actor_loss\", actor_loss.item(), ind_epoch)\n",
    "        \n",
    "    def save_models(self):\n",
    "        newpath = r'./models/'+ self.run_name +'/'+str(self.agent)\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "        torch.save(self.actor.state_dict(), newpath+\"/actor.pt\")\n",
    "        torch.save(self.qf1.state_dict(), newpath+\"/qf1.pt\")\n",
    "        torch.save(self.qf1_target.state_dict(), newpath+\"/qf1_target.pt\")\n",
    "        torch.save(self.target_actor.state_dict(), newpath+\"/target_actor.pt\")       \n",
    "    \n",
    "\n",
    "    def load_models(self):\n",
    "        newpath = r'./models/'+ self.run_name +'/'+str(self.agent)\n",
    "        self.actor.load_state_dict(torch.load(newpath+\"/actor.pt\"))\n",
    "        self.qf1.load_state_dict(torch.load(newpath+\"/qf1.pt\"))\n",
    "        self.qf1_target.load_state_dict(torch.load(newpath+\"/qf1_target.pt\"))\n",
    "        self.target_actor.load_state_dict(torch.load(newpath+\"/target_actor.pt\")) \n",
    "        \n",
    "\n",
    "                                          \n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607adb60-7e78-4345-915e-092f9836dc6f",
   "metadata": {},
   "source": [
    "## transfomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9df2b2af-4ca1-423e-8f69-02d71742af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\t\ts1,a1,r1,s2 = self.ram.sample(BATCH_SIZE)\n",
    "#\n",
    "#\t\ts1 = Variable(torch.from_numpy(s1))\n",
    "#\t\ta1 = Variable(torch.from_numpy(a1))\n",
    "#\t\tr1 = Variable(torch.from_numpy(r1))\n",
    "#\t\ts2 = Variable(torch.from_numpy(s2))\n",
    "#\n",
    "#\t\t# ---------------------- optimize critic ----------------------\n",
    "#\t\t# Use target actor exploitation policy here for loss evaluation\n",
    "#\t\ta2 = self.target_actor.forward(s2).detach()\n",
    "#\t\tnext_val = torch.squeeze(self.target_critic.forward(s2, a2).detach())\n",
    "#\t\t# y_exp = r + gamma*Q'( s2, pi'(s2))\n",
    "#\t\ty_expected = r1 + GAMMA*next_val\n",
    "#\t\t# y_pred = Q( s1, a1)\n",
    "#\t\ty_predicted = torch.squeeze(self.critic.forward(s1, a1))\n",
    "#\t\t# compute critic loss, and update the critic\n",
    "#\t\tloss_critic = F.smooth_l1_loss(y_predicted, y_expected)\n",
    "#\t\tself.critic_optimizer.zero_grad()\n",
    "#\t\tloss_critic.backward()\n",
    "#\t\tself.critic_optimizer.step()\n",
    "#\n",
    "#\t\t# ---------------------- optimize actor ----------------------\n",
    "#\t\tpred_a1 = self.actor.forward(s1)\n",
    "#\t\tloss_actor = -1*torch.sum(self.critic.forward(s1, pred_a1))\n",
    "#\t\tself.actor_optimizer.zero_grad()\n",
    "#\t\tloss_actor.backward()\n",
    "#\t\tself.actor_optimizer.step()\n",
    "#\n",
    "#\t\tutils.soft_update(self.target_actor, self.actor, TAU)\n",
    "#\t\tutils.soft_update(self.target_critic, self.critic, TAU)\n",
    "#z\n",
    "#\t\t# if self.iter % 100 == 0:\n",
    "#\t\t# \tprint 'Iteration :- ', self.iter, ' Loss_actor :- ', loss_actor.data.numpy(),#\t\t# \t\t' Loss_critic :- ', loss_critic.data.numpy()\n",
    "#\t\t# self.iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7eeccf5-0b86-485a-90f2-a453a209dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "#referrence for loss : https://github.com/vy007vikas/PyTorch-ActorCriticRL/blob/master/train.py\n",
    "\n",
    "#reference invalid actions ignore\n",
    "#https://ai.stackexchange.com/questions/2980/how-should-i-handle-invalid-actions-when-using-reinforce\n",
    "#ttps://arxiv.org/abs/2006.14171\n",
    "\n",
    "\n",
    "class transformer_model:\n",
    "\n",
    "    def __init__(  # ,config,args,hero = 1,\n",
    "        self,\n",
    "        qnet_config_dict,\n",
    "        actor_config_dict,\n",
    "        args,\n",
    "        device,\n",
    "        writer,\n",
    "        run_name,\n",
    "        agent,\n",
    "        ):\n",
    "\n",
    "        self.writer = writer\n",
    "        self.hero = agent\n",
    "        self.args = args\n",
    "        self.run_name = run_name\n",
    "        self.device = device  # config['device']\n",
    "        self.state_dim = actor_config_dict['ob_space']  # config['observation_space']#.shape[0]\n",
    "        self.act_dim = actor_config_dict['action_space']  # config['action_space']#.n #3#1 #env.action_space.shape[0]\n",
    "        self.n_blocks = args.model_config['n_blocks']\n",
    "        self.embed_dim = args.model_config['embed_dim']\n",
    "        self.context_len = args.model_config['context_len']\n",
    "        self.n_heads = args.model_config['n_heads']\n",
    "        self.dropout_p = args.model_config['dropout_p']\n",
    "        self.lr = args.learning_rate\n",
    "        self.wt_decay = args.model_config['wt_decay']\n",
    "        self.rb_len = args.model_config['rb_len']\n",
    "\n",
    "        # self.steps        =            config['steps']\n",
    "\n",
    "        self.warmup_epoch = args.model_config['warmup_epoch']\n",
    "        self.total_epoch = args.model_config['total_epoch']\n",
    "        self.initial_lr =  args.model_config['initial_lr'] #5e-4\n",
    "        self.final_lr =  args.model_config['final_lr'] #1e-6\n",
    "        \n",
    "        self.chunk_size = args.model_config['chunk_size']\n",
    "        self.chunk_overlap = args.model_config['chunk_overlap']\n",
    "        #self.max_d4rl_score = -1000.0\n",
    "        #self.total_updates = 0\n",
    "        self.tau = args.model_config['tau']\n",
    "        self.num_steps = args.num_steps\n",
    "        self.total_agents = args.total_agents\n",
    "        self.total_phases = args.total_phases\n",
    "        self.beta = args.model_config['beta']         #0.2 #Q_mse\n",
    "        self.alpha =args.model_config['alpha']          #0.1  #actionloss\n",
    "        self.entropy_coeff = args.model_config['entropy_coeff']         #0.1#0.5   #entropy loss in action\n",
    "        self.val_loss_coeff = args.model_config['val_loss_coeff']        #0.5      #Q loss\n",
    "        \n",
    "\n",
    "        # context_len_=200\n",
    "\n",
    "        self.model = DecisionTransformer(\n",
    "            state_dim=self.state_dim,\n",
    "            act_dim=self.act_dim,\n",
    "            n_blocks=self.n_blocks,\n",
    "            h_dim=self.embed_dim,\n",
    "            context_len=self.context_len,\n",
    "            n_heads=self.n_heads,\n",
    "            drop_p=self.dropout_p,\n",
    "            max_timestep=self.num_steps,\n",
    "            ).to(self.device)\n",
    "\n",
    "        self.target_model = DecisionTransformer(\n",
    "            state_dim=self.state_dim,\n",
    "            act_dim=self.act_dim,\n",
    "            n_blocks=self.n_blocks,\n",
    "            h_dim=self.embed_dim,\n",
    "            context_len=self.context_len,\n",
    "            n_heads=self.n_heads,\n",
    "            drop_p=self.dropout_p,\n",
    "            max_timestep=self.num_steps,\n",
    "            ).to(self.device)\n",
    "\n",
    "        # Set target network parameters to not require gradients\n",
    "        for param in self.target_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.copy_wt()\n",
    "        self.optimizer_2 = torch.optim.AdamW(self.model.parameters(),\n",
    "                lr=0.000005, weight_decay=self.wt_decay)  # lr,\n",
    "\n",
    "        # lr = 0.00001\n",
    "        \n",
    "        self.optimizer_1 = torch.optim.AdamW(self.model.parameters(),\n",
    "                lr=self.lr, weight_decay=self.wt_decay)  # lr,\n",
    "\n",
    "        #self.scheduler =             torch.optim.lr_scheduler.LambdaLR(self.optimizer_1, lambda steps: min((steps + 1) / self.warmup_steps, 1))\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer_1,T_max = self.total_epoch - self.warmup_epoch, eta_min=self.final_lr)\n",
    "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_1, T_0=150, T_mult=2, eta_min=0.01, last_epoch=-1)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def copy_wt(self):\n",
    "\n",
    "        # target_param.load_state_dict(param.state_dict())\n",
    "\n",
    "        for (param, target_param) in zip(self.model.parameters(),\n",
    "                self.target_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1\n",
    "                                    - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "    def action_loss_fn_3(\n",
    "        self,\n",
    "        timesteps,\n",
    "        states,\n",
    "        actions_1,actions_2,log_probs_actions_2,\n",
    "        returns_to_go,\n",
    "        return_preds_last,\n",
    "        returns_to_go_cal_last,\n",
    "        print_,\n",
    "        action_masks\n",
    "        ):\n",
    "\n",
    "                         # action_preds_2,\n",
    "                         # action_mask,\n",
    "                         # return_preds_2,\n",
    "                         # returns_target,\n",
    "                         # beta = 0.2\n",
    "\n",
    "        # model : calculate the action for the states\n",
    "        # create a new dataset with last action replaced by a_pred_module\n",
    "        # predict value of the action using model critic\n",
    "        # torch sum value....\n",
    "\n",
    "        # pred_a1 = self.actor.forward(s1)\n",
    "        # loss_actor = -1*torch.sum(self.critic.forward(s1, pred_a1))\n",
    "        # self.actor_optimizer.zero_grad()\n",
    "        # loss_actor.backward()\n",
    "        # self.actor_optimizer.step()\n",
    "\n",
    "        # so what should i do ..... hmmm ... yeah i only need time steps where agent is the hero agent...\n",
    "\n",
    "        ################################\n",
    "        #no epoch no re prediction .... \n",
    "        \n",
    "    \n",
    "        if True: # this does not look into old values\n",
    "            ( #_, \n",
    "             action_logit_model_1,#action_model_2_dir\n",
    "                _,_,logp_pi_a_2_dir,dist_entropy_a_1) =   self.model.forward(timesteps=timesteps, states=states,\n",
    "                                   actions_1=actions_1,actions_2=actions_2,\n",
    "                                   returns_to_go=returns_to_go, print_=2,return_logit=True,return_og_log_prob_a2=True)  # print_\n",
    "    \n",
    "                                                            # ,info = info\n",
    "    \n",
    "            actions_1_ = actions_1.clone().detach()#, requires_grad=False)\n",
    "    \n",
    "            # self.actions_ = actions_\n",
    "            # self.action_preds_model_ = action_preds_model\n",
    "            # actions_[:,:,-1] = action_preds_model\n",
    "    \n",
    "\n",
    "            log_probs_1 = -torch.nn.functional.cross_entropy(action_logit_model_1[:, -1, :], \n",
    "                                           actions_1_[:, -1].long(), reduction=\"none\")\n",
    "            \n",
    "            #(a1_[:,-1,:][hero_steps[:,-1,0]]*action_masks[:,-1,:][hero_steps[:,-1,0]])\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            #actions_1_[:, -1] =     ((action_logit_model_1[:, -1, :] + 0.000001*action_masks) *action_masks).argmax(axis=1).clone().detach() #i will fix this later, there is an issue.... when probabilities for all legal actions are 0, the product is same as illigal actions \n",
    "            actions_1_[:, -1] =     (action_logit_model_1[:, -1, :] *action_masks).argmax(axis=1).clone().detach() #i will fix this later, there is an issue.... when probabilities for all legal actions are 0, the product is same as illigal actions \n",
    "\n",
    "            ( #_, \n",
    "             _,#action_logit_model_2\n",
    "                _,_,logp_pi_a_2,_,) =   self.model.forward(timesteps=timesteps, states=states,\n",
    "                                   actions_1=actions_1_,actions_2=actions_2,\n",
    "                                   returns_to_go=returns_to_go, print_=2,return_logit=True,return_og_log_prob_a2=True)\n",
    "            \n",
    "\n",
    "            \n",
    "            #if False:\n",
    "            #with torch.no_grad():\n",
    "            #    ( #_, \n",
    "            #     _,_,values_) =   self.model.forward(timesteps=timesteps, states=states,\n",
    "            #                       actions_1=actions_1_,actions_2=actions_2,\n",
    "            #                       returns_to_go=returns_to_go, print_=2,return_logit=False)\n",
    "\n",
    "            advantages  = returns_to_go_cal_last - return_preds_last.clone().detach()\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            #if (not torch.isfinite(action_logit_model_1[:, -1,:]).all()) :\n",
    "            #    print('inf_error', 'action_logit_model_1')\n",
    "            #    print('action_logit_model_1',action_logit_model_1[:, -1,:])\n",
    "            #    print('log_probs_1',log_probs_1)\n",
    "            #    print('actions_1_',actions_1_[:, -1].long())\n",
    "            #    a()\n",
    "            #    \n",
    "            #if (not torch.isfinite(log_probs_1).all()):\n",
    "            #    print('inf_error', 'log_probs_1')\n",
    "            #    print('action_logit_model_1',action_logit_model_1[:, -1,:])\n",
    "            #    print('log_probs_1',log_probs_1)\n",
    "            #    print('actions_1_',actions_1_[:, -1].long())\n",
    "            #    a()\n",
    "            #    \n",
    "            #if (not torch.isfinite(actions_1_[:, -1].long() ).all())  :\n",
    "            #    print('inf_error','actions_1_')\n",
    "            #    print('action_logit_model_1',action_logit_model_1[:, -1,:])\n",
    "            #    print('log_probs_1',log_probs_1)\n",
    "            #    print('actions_1_',actions_1_[:, -1].long())\n",
    "            #    a()\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            try:\n",
    "                #the model is sending everyone .... 1. .... there must be an issue .... lower the weight\n",
    "                log_probs_2_dir = -torch.nn.BCELoss(reduction ='none')(\n",
    "\n",
    "                                               #logp_pi_a_2_dir[:, -1,0],\n",
    "                                               torch.clamp(logp_pi_a_2_dir[:, -1,0], min=0.00001, max=0.9999),\n",
    "                                               #torch.clamp(action_model_2_dir[:, -1,0], min=0.00001, max=0.9999), \n",
    "                                               torch.clamp(log_probs_actions_2[:, -1], min=0.00001, max=0.9999))\n",
    "\n",
    "                #if (not torch.isfinite(action_model_2_dir[:, -1,0]).all()) or (action_model_2_dir[:, -1,0] ==1).any() or (action_model_2_dir[:, -1,0] ==0).any():\n",
    "                #    print('inf_error', 'action_model_2_dir')\n",
    "                #    print('action_model_2_dir',action_model_2_dir[:, -1,0])\n",
    "                #    print('log_probs_2_dir',log_probs_2_dir)\n",
    "                #    print('actions_2',actions_2[:, -1])\n",
    "                #    a()\n",
    "                #    \n",
    "                #if (not torch.isfinite(log_probs_2_dir).all()):\n",
    "                #    print('inf_error', 'log_probs_1')\n",
    "                #    print('action_model_2_dir',action_model_2_dir[:, -1,0])\n",
    "                #    print('log_probs_2_dir',log_probs_2_dir)\n",
    "                #    print('actions_2',actions_2[:, -1])\n",
    "                #    a()\n",
    "                #\n",
    "                #if (not torch.isfinite(actions_2[:, -1]).all()):\n",
    "                #    print('inf_error', 'actions_2')\n",
    "                #    print('action_model_2_dir',action_model_2_dir[:, -1,0])\n",
    "                #    print('log_probs_2_dir',log_probs_2_dir)\n",
    "                #    print('actions_2',actions_2[:, -1])\n",
    "                #    a()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(logp_pi_a_2_dir[:, -1,0])\n",
    "                print(log_probs_actions_2[:, -1])\n",
    "                a()\n",
    "\n",
    "            log_probs_2 = -torch.nn.BCELoss(reduction ='none')(#logp_pi_a_2[:, -1,0],#action_logit_model_2[:, -1,0], \n",
    "                                                               torch.clamp(logp_pi_a_2[:, -1,0], min=0.00001, max=0.9999),\n",
    "                                           torch.clamp(log_probs_actions_2[:, -1], min=0.00001, max=0.9999)\n",
    "                                            )\n",
    "            \n",
    "            #if (not torch.isfinite(log_probs_2).all()):\n",
    "            #    #pass\n",
    "            #    print('inf_error', 'log_probs_2')\n",
    "            #    print('log_probs_2',log_probs_2)\n",
    "            #    a()\n",
    "\n",
    "            #if (not torch.isfinite(action_logit_model_2[:, -1,0]).all()) or (action_logit_model_2[:, -1,0] ==1).any() or (action_logit_model_2[:, -1,0] ==0).any():\n",
    "            #        print('inf_error', 'action_logit_model_2')\n",
    "            #        print('action_logit_model_2',action_logit_model_2[:, -1,0])\n",
    "            #        a()\n",
    "\n",
    "            #print(logp_pi_a_2_dir[:,-1,0],logp_pi_a_2[:,-1,0])\n",
    "\n",
    "            pi_loss = -((log_probs_1 +log_probs_2 + log_probs_2_dir)*(advantages)).mean() - self.entropy_coeff*dist_entropy_a_1.mean()\n",
    "\n",
    "            \n",
    "            \n",
    "            return pi_loss\n",
    "\n",
    "    def value_loss_fn_3(\n",
    "        self,\n",
    "        reward_last,\n",
    "        return_preds_last,\n",
    "        returns_target_last,\n",
    "        returns_to_go_cal_last,\n",
    "        beta=0.2,\n",
    "        alpha=2,\n",
    "        gamma=0.99,\n",
    "        device='cpu',\n",
    "        ):\n",
    "        \n",
    "        RT1 = reward_last[:-1] + gamma * returns_target_last[1:]\n",
    "        try:\n",
    "            Q_TD = torch.nn.functional.smooth_l1_loss(return_preds_last[:-1],RT1) \n",
    "            \n",
    "            Q_TD=Q_TD+torch.nn.functional.smooth_l1_loss(return_preds_last[[-1]], reward_last[[-1]])\n",
    "            \n",
    "            Q_MSE = torch.nn.functional.smooth_l1_loss(return_preds_last, returns_to_go_cal_last)\n",
    "            return (Q_TD ,  Q_MSE)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('in-v-loss')\n",
    "            print( 'return_preds_last' ,return_preds_last)\n",
    "            print('RT1' ,RT1)\n",
    "            print('reward_last' ,reward_last)\n",
    "            print('returns_to_go_cal_last' ,returns_to_go_cal_last)\n",
    "            a()\n",
    "\n",
    "        \n",
    "\n",
    "    def calculate_loss(\n",
    "        self,\n",
    "        timesteps,\n",
    "        returns_target, #target model predictions \n",
    "        returns_to_go_cal,  # Q calculated\n",
    "        hero_steps,#current_agent_acting,\n",
    "        states,\n",
    "        actions_1,actions_2,log_probs_actions_2,\n",
    "        return_preds,\n",
    "        return_preds_v        \n",
    "        ,reward, #actual rewards \n",
    "        returns_to_go,  #actual R2G given to the model\n",
    "        action_masks,\n",
    "        print_,\n",
    "        chunk_id,\n",
    "        divi\n",
    "        ):\n",
    "\n",
    "\n",
    "        \n",
    "        # only consider non padded elements\n",
    "\n",
    "        reward_last = reward[:, -1].squeeze().view(-1).to(self.device,dtype=torch.float32) #require the last reward only\n",
    "        return_preds_last = return_preds[:, -1,-1].squeeze().view(-1).to(self.device,dtype=torch.float32)  # act_dim , this is what out model predicted\n",
    "        return_preds_v_last = return_preds_v[:, -1,-1].squeeze().view(-1).to(self.device,dtype=torch.float32)  # act_dim , this is what out model predicted\n",
    "       \n",
    "        returns_target_last = returns_target[:, -1,-1].squeeze().view(-1).to(self.device,dtype=torch.float32) # target model prediction ... should be the 1st prediction\n",
    "       \n",
    "        returns_to_go_cal_last = returns_to_go_cal[:, -1,-1].squeeze().view(-1).to(self.device,dtype=torch.float32) # we need the Q value of the last action\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        hero_step_filter = hero_steps[:,-1,0] # current_agent_acting[:, -1, 0] == self.hero #last action is out heros\n",
    "\n",
    "        if sum(hero_step_filter) == 0:\n",
    "            policy_loss = torch.tensor(0,device=self.device)\n",
    "        else:\n",
    "            policy_loss = self.action_loss_fn_3(timesteps[hero_step_filter],\n",
    "                                                states[hero_step_filter],\n",
    "                                                actions_1[hero_step_filter],actions_2[hero_step_filter],log_probs_actions_2[hero_step_filter],\n",
    "                                                returns_to_go[hero_step_filter],\n",
    "                                                \n",
    "                                                return_preds_last[hero_step_filter],\n",
    "                                                \n",
    "                                                returns_to_go_cal_last[hero_step_filter], print_,\n",
    "                                               \n",
    "                                                action_masks[:,-1,:][hero_step_filter])\n",
    "\n",
    "        # but we need to figure out a little more\n",
    "\n",
    "\n",
    "\n",
    "        if  len(reward_last) == 0:\n",
    "            print('chunk_id',chunk_id)\n",
    "            print('reward',reward.shape)\n",
    "            print('reward_last',reward)\n",
    "            print('return_preds_last',return_preds_last.shape)\n",
    "\n",
    "\n",
    "            print('return_preds_last',return_preds_last)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        if True:  # self.value_cal_loss:\n",
    "            (Q_TD , Q_MSE) = self.value_loss_fn_3(  # value_loss_fn(return_preds_2,return_target,action_mask,beta=1)\n",
    "                                    reward_last,\n",
    "                                    return_preds_v_last,\n",
    "                                    returns_target_last,\n",
    "                                    returns_to_go_cal_last,\n",
    "                                    beta=0.5,\n",
    "                                    alpha=2,\n",
    "                                    gamma=0.99\n",
    "                                )\n",
    "\n",
    "            Q_TD = Q_TD/divi\n",
    "            Q_MSE = Q_MSE/divi\n",
    "            Q_loss = Q_TD + self.beta*Q_MSE\n",
    "            \n",
    "            policy_loss =  policy_loss/divi\n",
    "\n",
    "            \n",
    "            total_loss = self.val_loss_coeff*Q_loss + self.alpha*policy_loss\n",
    "\n",
    "            if (not torch.isfinite(policy_loss)) or (not torch.isfinite(Q_TD)) or (not torch.isfinite(Q_MSE)):\n",
    "                print(divi, 'policy_loss',policy_loss,'Q_TD',Q_TD,'Q_MSE',Q_MSE)\n",
    "                a()\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        return total_loss, (Q_TD, Q_MSE, Q_loss, policy_loss)\n",
    "\n",
    "    def train_write(self, iteration, print_=False):\n",
    "\n",
    "        total_loss_list = []\n",
    "        Q_TD_list = []\n",
    "        Q_MSE_list = []\n",
    "        Q_loss_list = []\n",
    "        policy_loss_list = []\n",
    "\n",
    "        if self.args.TB_log:\n",
    "            self.writer.add_scalar(\"charts/learning_rate\", self.optimizer_1.param_groups[0][\"lr\"], iteration)        \n",
    "        \n",
    "        for epoch in range(self.args.update_epochs):\n",
    "            for i,batch in enumerate(self.traj_data_loader):\n",
    "                total_loss = 0\n",
    "                Q_TD = 0\n",
    "                Q_MSE = 0\n",
    "                Q_loss = 0\n",
    "                policy_loss = 0\n",
    "                print(i)\n",
    "\n",
    "\n",
    "                batch_len = batch[0].shape[1]#2840\n",
    "                \n",
    "                if batch_len > self.chunk_size:\n",
    "                    a_ = [(i,i+self.chunk_size) for i in range(0, batch_len     -self.chunk_size,self.chunk_size-self.chunk_overlap)  ]\n",
    "                    a_  =a_+[ (a_[-1][1] -self.chunk_overlap ,batch_len) ]\n",
    "                else:\n",
    "                    a_ = [(0,batch_len)]\n",
    "    \n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                divi = len(a_)\n",
    "\n",
    "                \n",
    "                print('divi',divi,'chunk_size',self.chunk_size,'batch_shape',batch[0].shape)\n",
    "                \n",
    "                for (chunk_id, i) in enumerate(a_):#range(0,batch[0].shape[1] - self.chunk_size + 1,self.chunk_size - self.chunk_overlap)):\n",
    "\n",
    "                    # print(i,(i + self.chunk_size))\n",
    "                    total_loss_chunk, Q_TD_chunk, Q_MSE_chunk, Q_loss_chunk, policy_loss_chunk = self.train_write_smaller_chunk((tens[:, i[0]:i[1]] for tens in batch),\n",
    "                                                                                                                                    iteration, epoch, chunk_id, print_=print_,divi=divi)\n",
    "\n",
    "                    \n",
    "                    total_loss= total_loss + total_loss_chunk\n",
    "                    Q_TD = Q_TD+ Q_TD_chunk\n",
    "                    Q_MSE = Q_MSE+ Q_MSE_chunk\n",
    "                    Q_loss = Q_loss+ Q_loss_chunk\n",
    "                    policy_loss = policy_loss+ policy_loss_chunk\n",
    "\n",
    "                total_loss_list.append(total_loss)\n",
    "                Q_TD_list.append(Q_TD)\n",
    "                Q_MSE_list.append(Q_MSE)\n",
    "                Q_loss_list.append(Q_loss)\n",
    "                policy_loss_list.append(policy_loss)            \n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "        \n",
    "        if iteration < self.warmup_epoch:\n",
    "            # Linear warmup: Gradually increase learning rate during warmup\n",
    "            lr = self.initial_lr * iteration / self.warmup_epoch\n",
    "            for param_group in self.optimizer_1.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        else:\n",
    "            self.scheduler.step()\n",
    "\n",
    "        \n",
    "        print(\"total_loss\", np.mean(total_loss_list),\n",
    "              \"Q_TD\", np.mean(Q_TD_list),\n",
    "                \"Q_MSE\", np.mean(Q_MSE_list),\n",
    "                \"Q_loss\", np.mean(Q_loss_list),\n",
    "                \"policy_loss\", np.mean(policy_loss_list))\n",
    "\n",
    "        if self.args.TB_log:\n",
    "        \n",
    "            self.writer.add_scalar(\"total_loss\", np.mean(total_loss_list), iteration)\n",
    "            self.writer.add_scalar(\"Q_TD\", np.mean(Q_TD_list), iteration)\n",
    "            self.writer.add_scalar(\"Q_MSE\", np.mean(Q_MSE_list), iteration)\n",
    "            self.writer.add_scalar(\"Q_loss\", np.mean(Q_loss_list), iteration)\n",
    "            self.writer.add_scalar(\"policy_loss\", np.mean(policy_loss_list), iteration)    \n",
    "                    \n",
    "                \n",
    "                \n",
    "        \n",
    "        self.copy_wt()\n",
    "\n",
    "    def train_write_smaller_chunk(\n",
    "        self,\n",
    "        data,\n",
    "        iteration,\n",
    "        epoch,\n",
    "        chunk_id,\n",
    "        print_=False,divi = 1\n",
    "        ):\n",
    "\n",
    "        (\n",
    "            timesteps,\n",
    "            states,\n",
    "            actions_1,actions_2,log_probs_actions_2,\n",
    "            returntogo,\n",
    "            returns_to_go_cal,\n",
    "            returntogo_pred,\n",
    "            reward,\n",
    "            traj_mask,\n",
    "            action_masks,\n",
    "            current_agent_acting,\n",
    "            current_agent_simple,\n",
    "            current_agent,\n",
    "            current_phase,\n",
    "            current_troops_count,\n",
    "            ) = data\n",
    "\n",
    "\n",
    "        if len(timesteps[0].shape) ==0:\n",
    "            print( timesteps.shape )\n",
    "\n",
    "        \n",
    "        timesteps = timesteps[0].to(self.device)  # B x T\n",
    "        states = states[0].to(self.device)  # B x T x state_dim\n",
    "        actions_1 = actions_1[0].to(self.device)  # B x T x act_dim\n",
    "        actions_2 = actions_2[0].to(self.device)  # B x T x act_dim\n",
    "        log_probs_actions_2 = log_probs_actions_2[0].to(self.device)\n",
    "        reward = reward[0].to(self.device)\n",
    "        returns_to_go_cal =             returns_to_go_cal[0].to(self.device).unsqueeze(dim=-1)  # B x T x 1\n",
    "        returntogo = returntogo[0].to(self.device)\n",
    "        returntogo_pred = returntogo_pred[0].to(self.device)\n",
    "        traj_mask = traj_mask[0].to(self.device)  # B x T\n",
    "        action_masks = action_masks[0].to(self.device)\n",
    "        current_agent_acting = current_agent_acting[0].to(self.device)\n",
    "        current_agent_simple = current_agent_simple[0].to(self.device)\n",
    "        current_agent = current_agent[0].to(self.device)\n",
    "        current_phase = current_phase[0].to(self.device)\n",
    "        current_troops_count = current_troops_count[0].to(self.device)\n",
    "\n",
    "        info = dict({})\n",
    "\n",
    "\n",
    "        \n",
    "        hero_steps = current_agent_simple == self.hero\n",
    "\n",
    "        states = torch.cat((states, action_masks * hero_steps,\n",
    "                              current_phase, current_agent,\n",
    "                              current_troops_count[:, :, None]), axis=2)  # ,torch.ones(len(action_masks))[:,None]*self.hero\n",
    "\n",
    "\n",
    "\n",
    "        #print(actions_1.requires_grad,actions_2.requires_grad)\n",
    "\n",
    "        (action_pred_1,action_pred_2) =  (actions_1* hero_steps[:,0],\n",
    "                                                        actions_2* hero_steps[:,0]\n",
    "                                                         )\n",
    "\n",
    "        # state_preds, action_preds, return_preds = self.model.forward(\n",
    "        #                                                timesteps=timesteps,\n",
    "        #                                                states=states,\n",
    "        #                                                actions=actions,\n",
    "        #                                                returns_to_go=returns_to_go\n",
    "        #                                                #,info = info\n",
    "        #                                            )\n",
    "\n",
    "        #so this return to go is for value loss estimation\n",
    "\n",
    "        _,_, returntogo_pred_v,_ = self.model.forward(\n",
    "                                                        timesteps=timesteps,\n",
    "                                                        states=states,\n",
    "                                                        actions_1=action_pred_1,actions_2=action_pred_2,\n",
    "                                                        returns_to_go=returntogo,\n",
    "                                                            print_=print_\n",
    "                                                        #,info = info\n",
    "                                                    )\n",
    "        \n",
    "        # this is also for value function loss estimation\n",
    "\n",
    "        (#state_preds_target, \n",
    "         _,_,return_preds_target,_) =  self.target_model.forward(timesteps=timesteps,\n",
    "                                                                                states=states, actions_1=action_pred_1,actions_2=action_pred_2,\n",
    "                                                                                returns_to_go=returntogo, print_=print_)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        (#state_preds_target, \n",
    "         return_preds_target) = (return_preds_target.detach())\n",
    "\n",
    "        #if (not torch.isfinite(return_preds_target[:, -1]).all()\n",
    "        #   ) or (not torch.isfinite(returntogo_pred_v[:, -1]).all()\n",
    "        #        ) or (not torch.isfinite(action_pred_1[:, -1]).all()\n",
    "        #             ) or (not torch.isfinite(action_pred_2[:, -1]).all()\n",
    "        #                  )     :\n",
    "        #    \n",
    "        #    #print(action_model_2_dir[:, -1,0])\n",
    "        #    \n",
    "        #    print('returntogo_pred_v',returntogo_pred_v[:, -1])\n",
    "        #    print('return_preds_target',return_preds_target[:, -1])\n",
    "        #    \n",
    "        #    print('action_pred_2',action_pred_2[:, -1])\n",
    "        #    print('action_pred_1',action_pred_1[:, -1].long())\n",
    "        #    a()\n",
    "\n",
    "        #if True:\n",
    "        #    print('traj_mask',traj_mask.requires_grad)\n",
    "        #    print('actions',actions.requires_grad)\n",
    "        #    print('returntogo_pred',returntogo_pred.requires_grad)\n",
    "        #    print('reward', reward.requires_grad)\n",
    "        #    print('returntogo',returntogo.requires_grad)\n",
    "        #    print('returns_to_go_cal',returns_to_go_cal.requires_grad)\n",
    "        #    print('hero_steps',hero_steps.requires_grad)\n",
    "        #    a()\n",
    "        if  len(reward[:, -1].squeeze().view(-1).shape) == 0:\n",
    "            \n",
    "            print('return_preds_last',return_preds_target.shape)\n",
    "            print('reward_last',reward.shape)\n",
    "            print('returns_to_go_cal_last',returns_to_go_cal.shape)\n",
    "\n",
    "            print('return_preds_last',return_preds_target)\n",
    "            print('reward_last',reward)\n",
    "            print('returns_to_go_cal_last',returns_to_go_cal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        total_loss, (Q_TD, Q_MSE, Q_loss, policy_loss) = self.calculate_loss(\n",
    "                                                                                timesteps,\n",
    "                                                                                return_preds_target,#target model predictions \n",
    "                                                                                returns_to_go_cal, # Q calculated\n",
    "                                                                                hero_steps    ,#current_agent_acting,\n",
    "                                                                                states,\n",
    "                                                                                actions_1,actions_2,log_probs_actions_2,\n",
    "                                                                                returntogo_pred,\n",
    "                                                                                returntogo_pred_v,\n",
    "                                                                                reward,#actual rewards \n",
    "                                                                                returntogo,  #actual R2G given to the model\n",
    "                                                                                action_masks,\n",
    "                                                                                print_,\n",
    "                                                                                chunk_id,divi\n",
    "                                                                                )\n",
    "        #if chunk_id == 0:\n",
    "        #    print (chunk_id, total_loss)\n",
    "\n",
    "        # action_loss = #nn.CrossEntropyLoss().forward(action_preds_2,action_target)#F.mse_loss(action_preds_2, action_target.float(), reduction='mean')\n",
    "\n",
    "        self.optimizer_1.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.25)\n",
    "        self.optimizer_1.step()\n",
    "        #self.scheduler.step()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # policy_loss.backward(retain_graph=True)\n",
    "\n",
    "        return total_loss.detach().item() ,Q_TD.detach().item(), Q_MSE.detach().item(), Q_loss.detach().item(), policy_loss.detach().item()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        # if value_cal_loss:\n",
    "        #    q_los = Q_loss.detach().cpu().item()\n",
    "        #    writer.add_scalar(\"Q_loss\", q_los, iyr+i_train_iter*num_updates_per_iter)\n",
    "        #    log_Q_losses.append(q_los)\n",
    "        #\n",
    "        # pol_los = policy_loss.detach().cpu().item()\n",
    "        # writer.add_scalar(\"Policy_loss\", pol_los, iyr+i_train_iter*num_updates_per_iter)\n",
    "        # log_action_losses.append(pol_los)\n",
    "\n",
    "        # # evaluate on env\n",
    "        # results = evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale,\n",
    "        #                        num_eval_ep, max_eval_ep_len, state_mean, state_std,\n",
    "        # ........................)\n",
    "        #\n",
    "        # writer.add_scalar(\"eval/avg_reward\", results['eval/avg_reward'], i_train_iter)\n",
    "        # writer.add_scalar(\"eval/avg_ep_len\", results['eval/avg_ep_len'], i_train_iter)\n",
    "        # writer.add_scalar(\"eval/'total_reward\",results['total_reward'],i_train_iter)\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        # eval_avg_reward = results['eval/avg_reward']\n",
    "        # eval_avg_ep_len = results['eval/avg_ep_len']\n",
    "        # eval_d4rl_score = results['eval/avg_reward']#get_d4rl_normalized_score(results['eval/avg_reward'], env_name) * 100\n",
    "        # mean_action_loss,mean_Q_loss = np.mean(log_action_losses),np.mean(log_Q_losses)\n",
    "        #\n",
    "        # writer.add_scalar(\"mean_action_loss_per_iter\", mean_action_loss, i_train_iter)\n",
    "        # writer.add_scalar(\"mean_Q_loss_per_iter\", mean_Q_loss, i_train_iter)\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        # time_elapsed = str(datetime.now().replace(microsecond=0) - start_time)\n",
    "        # total_updates += num_updates_per_iter\n",
    "        # log_str = (\"=\" * 60 + '\\n' +\n",
    "        # ........\"time elapsed: \" + time_elapsed  + '\\n' +\n",
    "        # ........\"num of updates: \" + str(total_updates) + '\\n' +\n",
    "        # ........\"action loss: \" +  format(mean_action_loss, \".5f\") + '\\n' +\n",
    "        # ........\"Q loss: \" +  format(mean_Q_loss, \".5f\") + '\\n' +\n",
    "        # ........\"eval avg reward: \" + format(eval_avg_reward, \".5f\") + '\\n' +\n",
    "        # ........\"eval avg ep len: \" + format(eval_avg_ep_len, \".5f\") + '\\n' +\n",
    "        # ........\"eval d4rl score: \" + format(eval_d4rl_score, \".5f\") +'\\n'+\n",
    "        #        \" iterations: \"+str(i_train_iter) +\" calulate_value_loss :\" +str(value_cal_loss)\n",
    "        # ........)\n",
    "        #\n",
    "        # print(log_str)\n",
    "        #\n",
    "        # log_data = [time_elapsed, total_updates, mean_action_loss,mean_Q_loss,\n",
    "        # ............eval_avg_reward, eval_avg_ep_len,\n",
    "        # ............eval_d4rl_score]\n",
    "        #\n",
    "        # csv_writer.writerow(log_data)\n",
    "        #\n",
    "        # # save model\n",
    "        # print(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n",
    "        # if eval_d4rl_score >= max_d4rl_score:\n",
    "        #    print(\"saving max d4rl score model at: \" + save_best_model_path)\n",
    "        #    torch.save(model.state_dict(), save_best_model_path)\n",
    "        #    max_d4rl_score = eval_d4rl_score\n",
    "        #\n",
    "        # print(\"saving current model at: \" + save_model_path)\n",
    "        # torch.save(model.state_dict(), save_model_path)\n",
    "\n",
    "        # writer.add_hparams(dict(exp_name=log_csv_name,env_name=env_name,state_dim=state_dim,\n",
    "        # ............act_dim=int(act_dim),\n",
    "        # ............n_blocks=n_blocks,\n",
    "        # ............h_dim=embed_dim,\n",
    "        # ............context_len=context_len,\n",
    "        # ............n_heads=n_heads,\n",
    "        # ............drop_p=dropout_p,training_division=training_division,\n",
    "        #            start_value_training_iteration=start_value_training_iteration,optim=optimizer_1.__class__.__name__,lr=lr,\n",
    "        #                       scheduler=scheduler.__class__.__name__, sch_param = \",\".join([ i+\":\"+str(j) for i,j in dict(T_0=150, T_mult=2, eta_min=0.01, last_epoch=-1).items()])\n",
    "        #                       ,batch_size=batch_size),{'hparam/eval_avg_reward':eval_avg_reward})\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "        # print(\"finished training!\")\n",
    "        # print(\"=\" * 60)\n",
    "        # end_time = datetime.now().replace(microsecond=0)\n",
    "        # time_elapsed = str(end_time - start_time)\n",
    "        # end_time_str = end_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "        # print(\"started training at: \" + start_time_str)\n",
    "        # print(\"finished training at: \" + end_time_str)\n",
    "        # print(\"total training time: \" + time_elapsed)\n",
    "        # print(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n",
    "        # print(\"saved max d4rl score model at: \" + save_best_model_path)\n",
    "        # print(\"saved last updated model at: \" + save_model_path)\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "    def init_path(self):\n",
    "        self.paths = []\n",
    "\n",
    "    def init_CL_sample_store(self):\n",
    "        self.DT_input = {  # self.actor_config_dict['ob_space']\n",
    "            'timestep': torch.zeros((1,\n",
    "                                    self.context_len),requires_grad =False).to(self.device,\n",
    "                    dtype=torch.int),\n",
    "            'state': torch.zeros((1, self.context_len,\n",
    "                                 self.state_dim),requires_grad =False).to(self.device,dtype=torch.float32),\n",
    "            'action_1': torch.zeros((1, self.context_len\n",
    "                                  )).to(self.device,dtype=torch.float32),\n",
    "            'action_2': torch.zeros((1, self.context_len\n",
    "                                  )).to(self.device,dtype=torch.float32),\n",
    "            \n",
    "            'return_to_go': torch.ones((1,\n",
    "                    self.context_len),requires_grad =False).to(self.device,dtype=torch.float32) * 110,\n",
    "            }\n",
    "\n",
    "        self.returntogo = torch.zeros((self.num_steps,\n",
    "                1),requires_grad =False).to(self.device,dtype=torch.float32)  # self.total_agents\n",
    "        self.returntogo_pred = torch.zeros((self.num_steps,\n",
    "                1)).to(self.device,dtype=torch.float32)  # self.total_agents\n",
    "\n",
    "    #here\n",
    "    def current_model_in(\n",
    "        self,\n",
    "        observation,\n",
    "        curr_agent,\n",
    "        phase_mapping,\n",
    "        curr_agent_mapping,\n",
    "        env_board_agents=[],\n",
    "        ):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.model_in =  torch.hstack((observation['observation'\n",
    "                         ].reshape(-1).to(self.device),\n",
    "                         torch.tensor(observation['action_mask'\n",
    "                         ].reshape(-1)).to(self.device) * (curr_agent\n",
    "                         == self.hero), phase_mapping.to(self.device),\n",
    "                         curr_agent_mapping.to(self.device),\n",
    "                         ( (torch.tensor([env_board_agents[self.hero].bucket]).to(self.device) - 5.2496)/1.4733\n",
    "                                      )))[None,\n",
    "                         :].float().requires_grad_(False).to(self.device)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def update_CL_sample_store(\n",
    "        self,\n",
    "        curr_agent_,\n",
    "        inp={'step': None, 'act_2_1': [],'act_2_2': [], 'curr_reward_list': []},\n",
    "        before_action=True,\n",
    "        ):\n",
    "\n",
    "        if before_action == 1 :\n",
    "\n",
    "            if inp['step'] == 0:\n",
    "\n",
    "                # print(self.model_in.shape)\n",
    "                # print(self.model_in.repeat(self.context_len).shape)\n",
    "\n",
    "                self.DT_input['state'] = self.model_in.repeat(self.context_len,\n",
    "                        1).to(self.device)[None, :]\n",
    "            else:\n",
    "\n",
    "            # if step<self.context_len:\n",
    "\n",
    "            #    trace[step] = model_in\n",
    "\n",
    "                self.DT_input['state'][0, 0:-1] = self.DT_input['state'\n",
    "                        ][0, 1:].clone()\n",
    "                self.DT_input['state'][0, -1] = self.model_in\n",
    "                self.DT_input['timestep'][0, 0:-1] = self.DT_input['timestep'][0, 1:].clone()\n",
    "                self.DT_input['timestep'][0, -1] = inp['step']\n",
    "                self.DT_input['action_1'][0, 0:-1] = self.DT_input['action_1'][0, 1:].clone()\n",
    "                self.DT_input['action_2'][0, 0:-1] = self.DT_input['action_2'][0, 1:].clone()\n",
    "                \n",
    "                self.DT_input['return_to_go'][0, 0:-1] = self.DT_input['return_to_go'][0, 1:].clone()\n",
    "        elif before_action == 2 :\n",
    "            if self.hero == curr_agent_:\n",
    "                self.DT_input['action_1'][0, -1] = inp['act_2_1']\n",
    "            else:\n",
    "                self.DT_input['action_1'][0, -1] = 0\n",
    "\n",
    "        elif before_action == 3  :\n",
    "            if self.hero == curr_agent_:\n",
    "                self.DT_input['action_2'][0, -1] = inp['act_2_2']\n",
    "            else:\n",
    "                self.DT_input['action_2'][0, -1] = 0\n",
    "        else:\n",
    "                \n",
    "            self.DT_input['return_to_go'][0, -1] = self.DT_input['return_to_go'][0, -1] -    inp['curr_reward_list']  # [self.hero]\n",
    "            self.returntogo[inp['step']] = self.DT_input['return_to_go'\n",
    "                    ][0, -1]\n",
    "\n",
    "    def update_train_data(\n",
    "        self,\n",
    "        step_count,\n",
    "        obs,\n",
    "        ob_space_shape,\n",
    "        rewards_2,\n",
    "        dones_2,\n",
    "        actions_1,\n",
    "        actions_2,\n",
    "        log_probs_actions_2,\n",
    "        action_masks,\n",
    "        current_agent,\n",
    "        current_agent_acting,\n",
    "        current_phase,\n",
    "        current_troops_count,\n",
    "        map_agent_phase_vector,\n",
    "        ):\n",
    "\n",
    "        \n",
    "        \n",
    "        data_ = collections.defaultdict(torch.tensor)\n",
    "        data_['observations'] =             obs[:step_count].reshape(-1,\n",
    "                np.prod(ob_space_shape))\n",
    "\n",
    "                # data_['next_observations'] = obs[1:step_count+1].to(device =self.args.pin_memory_device).reshape(-1,np.prod(T.ob_space_shape)) #torch.tensor([1,2,3,4])\n",
    "                # this return to go is the actual input\n",
    "\n",
    "        data_['returntogo'] =             self.returntogo[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['returntogo_pred'] =             self.returntogo_pred[:step_count]  # torch.tensor([1,2,3,4])\n",
    "\n",
    "        data_['rewards'] =             rewards_2[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['terminals'] =             dones_2[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['actions_1'] =             actions_1[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['actions_2'] =             actions_2[:step_count]  # torch.tensor([1,2,3,4])\n",
    "        data_['log_probs_actions_2'] =   log_probs_actions_2[:step_count]\n",
    "        data_['action_masks'] =             action_masks[:step_count]\n",
    "        data_['current_agent_acting'] =     current_agent_acting[:step_count]\n",
    "        data_['current_agent_simple'] =     current_agent[:step_count]\n",
    "        data_['current_agent'] =             map_agent_phase_vector(current_agent[:step_count],\n",
    "                                   num_classes=self.total_agents + 1)[:\n",
    "                , 1:]\n",
    "        data_['current_phase'] =             map_agent_phase_vector(current_phase[:step_count],\n",
    "                                   num_classes=self.total_phases)\n",
    "        data_['current_troops_count'] =             current_troops_count[:step_count]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        self.datase = TrajectoryDataset_per_episode([data_], #len(data[0]['observations'])<self.args.context_len\n",
    "                context_len=self.args.context_len,rtg_scale=self.args.rtg_scale,dev = self.args.pin_memory_device,\n",
    "                 gamma=self.args.gamma)\n",
    "\n",
    "        self.path_que(DataLoader(self.datase, batch_size=len(self.datase)))\n",
    "    def path_que(self, dtl):\n",
    "        \n",
    "        if (len(self.paths)==self.rb_len):\n",
    "            if (self.rb_len >1):\n",
    "                self.paths.pop(random.randrange(len(self.paths)-self.args.num_episodes +1 )) # dont pop the most recent experiences and ensure rb_len > num.episodes\n",
    "            else: \n",
    "                self.paths.pop()\n",
    "        self.paths.append(dtl)\n",
    "        \n",
    "    def create_training_dataset(self):\n",
    "        self.traj_dataset =  TrajectoryDataset_2_through_episodes(self.paths)  # a dataset of dataloaders\n",
    "\n",
    "        self.traj_data_loader = DataLoader(  # only spit 1 episode a time\n",
    "            self.traj_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=self.args.shuffle,\n",
    "            pin_memory=self.args.pin_memory,\n",
    "            drop_last=self.args.drop_last,\n",
    "            pin_memory_device=self.args.pin_memory_device,\n",
    "            )\n",
    "\n",
    "    def action_predict(self, save_R=True, return_R=False,shift=1,action_masks = [],return_log_prob_a2 = False): \n",
    "        (#s, \n",
    "         a_1,a_2, R,lp) = self.model(timesteps=self.DT_input['timestep'],\n",
    "                               states=self.DT_input['state'],\n",
    "                               actions_1=self.DT_input['action_1'],\n",
    "                               actions_2=self.DT_input['action_2'],\n",
    "                               returns_to_go=self.DT_input['return_to_go'\n",
    "                               ][:, :, None], \n",
    "                                return_log_prob_a2 = return_log_prob_a2)\n",
    "\n",
    "\n",
    "        took_action = False\n",
    "        \n",
    "        if len(action_masks) > 0:\n",
    "            #handling when all the probability of masked actions are zero.... #have to force the model to pick 1st valid action.\n",
    "            masked_action = (a_1[0, -1, :]*action_masks)\n",
    "            #valid_ind = torch.nonzero(action_masks).squeeze()\n",
    "            if torch.any(masked_action !=0):\n",
    "                took_action = True\n",
    "                \n",
    "                action_1 = (masked_action).argmax()[None]\n",
    "            else: #im hoping that slowly and steadily the model would learn to predict non zero probabilities for action mask\n",
    "                \n",
    "                action_1 = torch.tensor(np.random.choice(torch.where(action_masks)[0]))[None]  #(masked_action + 0.0000001*action_masks).argmax()[None]\n",
    "        else:\n",
    "            action_1 = a_1[0, -1, :].argmax()[None]\n",
    "            \n",
    "        action_2 = a_2[0,-1, 0][None]\n",
    "        if return_log_prob_a2:\n",
    "            log_prob = lp[0,-1, 0][None]\n",
    "        else:\n",
    "            log_prob = None\n",
    "        \n",
    "        if save_R:\n",
    "            self.returntogo_pred[self.DT_input['timestep'][0, -1]] =                 R[0, -1]  # R\n",
    "\n",
    "        if return_R:\n",
    "            return (action_1, action_2, R[0, -1],took_action,log_prob)\n",
    "        else:\n",
    "            return action_1, action_2,took_action,log_prob\n",
    "\n",
    "    def action_predict_direct(self, data, return_R=False,return_log_prob_a2 = False):\n",
    "        (timesteps, states, actions_1,actions_2, returns_to_go) = data\n",
    "        (#s, \n",
    "         a_1,a_2, R,lp) = self.model(timesteps=timesteps, states=states,\n",
    "                               actions_1=actions_1,actions_2=actions_2,\n",
    "                               returns_to_go=returns_to_go,return_log_prob_a2 = return_log_prob_a2)\n",
    "        if return_R:\n",
    "            return (a_1,a_2, R,lp)\n",
    "        else:\n",
    "            return a_1,a_2,lp\n",
    "\n",
    "    def save_models(self):\n",
    "        newpath = r'./models/' + self.run_name + '/' + str(self.hero)\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "        torch.save(self.model.state_dict(), newpath\n",
    "                   + '/a2c_transformer.pt')\n",
    "\n",
    "    def load_models(self):\n",
    "        newpath = r'./models/' + self.run_name + '/' + str(self.hero)\n",
    "        self.model.load_state_dict(torch.load(newpath\n",
    "                                   + '/a2c_transformer.pt'))\n",
    "        self.target_model.load_state_dict(torch.load(newpath\n",
    "                + '/a2c_transformer.pt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aaf5a6-fc6c-4577-9741-911a5bc4c52b",
   "metadata": {},
   "source": [
    "## model selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf965ae-3b67-4972-9669-e7cada366dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selector(model_name=\"DDQN_module\", kwarg = {}):\n",
    "    model_list = {\"DDQN_module\":DDQN_module,\"transformer_model\":transformer_model}\n",
    "    print(type(model_list[model_name]))\n",
    "    return model_list[model_name](**kwarg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56145934-9391-473e-802f-92b9eb8f38e3",
   "metadata": {},
   "source": [
    "# hero agent definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2661a5d-7bae-4be9-97ae-cf181efef1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Hero_agent(int):\n",
    "    def init_properties(self,agent_count,phases,cp=[],df=[],direct_action=True):\n",
    "        #self.draw_count = 0\n",
    "        self.init_win_count_iter(agent_count)\n",
    "        self.init_move_count_epi(phases)\n",
    "        self.cp = cp\n",
    "        self.df = df\n",
    "        self.direct_action = direct_action\n",
    "        self.init_reward_concern(agent_count,cp=cp,df=df)\n",
    "        \n",
    "    def init_reward_concern(self,agent_count,cp=[],df=[]):\n",
    "        if len(cp)==0:\n",
    "            cp = [int(self)]\n",
    "        self.concern=torch.tensor([(1 if i in cp \n",
    "                             else \n",
    "                             (-1 if i in df \n",
    "                                  else 0)) for i in range(1,agent_count+1) ])\n",
    "        #self.concern_2 = self.concern\n",
    "        #self.concern_2[self-1] =0\n",
    "        \n",
    "        self.multi_dependency = (sum(self.concern !=0)>1)\n",
    "        \n",
    "        \n",
    "    def init_model(self,model_name=\"DDQN_module\",\n",
    "                   kwarg = dict({})):\n",
    "        self.model = model_selector(model_name=model_name, \n",
    "                                    kwarg = kwarg)\n",
    "\n",
    "        \n",
    "    def init_win_count_iter(self,agent_count):\n",
    "        self.count_dict = {i:0 for i in range(1,agent_count+1)}\n",
    "        self.count_draw_dict = {i:0 for i in range(1,agent_count+1)}\n",
    "        self.draw_territory_count = 0\n",
    "    def init_move_count_epi(self,phases):\n",
    "        self.bad_move_count = 0\n",
    "        self.bad_move_phase_count = {i:0 for i in phases}\n",
    "        self.move_count =  {i:0 for i in phases}        \n",
    "    \n",
    "    def model_def(self, model):\n",
    "        self.model =model\n",
    "\n",
    "    def action_predict(self,save_R=True,return_R = False,action_masks = [],return_log_prob_a2 = False):\n",
    "        return self.model.action_predict(save_R=save_R,return_R = return_R, action_masks = action_masks,return_log_prob_a2 = return_log_prob_a2)\n",
    "\n",
    "    def action_predict_direct(self,data,return_R = False,return_log_prob_a2 = False):\n",
    "        return self.model.action_predict_direct(data,return_R = return_R,return_log_prob_a2 = return_log_prob_a2)\n",
    "    def save_models(self):\n",
    "        self.model.save_models()\n",
    "\n",
    "    def process_reward(self,rewards,step,hero_steps):\n",
    "        if self.multi_dependency and self.direct_action:\n",
    "            return (rewards*self.concern.to(rewards.device)).sum(-1)[:step][hero_steps][:,None]\n",
    "        elif self.multi_dependency and not self.direct_action:\n",
    "            base_rew = torch.zeros( rewards[:step,self-1][hero_steps].shape,require_grad=False)\n",
    "            #print(base_rew)\n",
    "\n",
    "            \n",
    "            hero_step_list  = np.arange(0,step)[hero_steps]\n",
    "            for i,j in zip(hero_step_list[:-1],hero_step_list[1:]):\n",
    "                if j-i>1:\n",
    "                    #print(j,i,rewards[i:j],(rewards[i:j]*self.concern),(rewards[i:j]*self.concern).sum())\n",
    "                    base_rew[i]+= (rewards[i:j]*self.concern).sum()\n",
    "            #print(base_rew,rewards[hero_step_list[-1]:],(rewards[hero_step_list[-1]:]*self.concern))\n",
    "            base_rew[-1]+= (rewards[hero_step_list[-1]:]*self.concern).sum()\n",
    "            \n",
    "            return base_rew[:,None]\n",
    "            \n",
    "        else:\n",
    "            return rewards[:step][hero_steps][:,None]\n",
    "    \n",
    "    #def model_forward_call(self,name,kwarg):\n",
    "    #    return self.model_dict[name](**kwarg)\n",
    "        \n",
    "\n",
    "a = Hero_agent(1)\n",
    "a.init_properties(3,[1,2,3],cp=[1],df=[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64599fd5-357b-443a-a628-2edfdf2aca12",
   "metadata": {},
   "source": [
    "# dataset definition \n",
    "## episode trajectory\n",
    "## dataloader for per iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab166bbf-5fcf-4a73-9b0b-8fc925cff456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs[:,-1] = (obs[:,-1]  - 5.2496)/1.4733\n",
    "\n",
    "#        current_troops_count_mean tensor(5.2496), current_troops_count_std = tensor(1.4733)\n",
    "\n",
    "#        (x-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89b4d63e-4634-453e-99dc-515bdd9b6f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "class TrajectoryDataset_per_episode(Dataset): #this should have only 1 trajectory no matter what\n",
    "    def __init__(self, trajectories, context_len, rtg_scale,dev,gamma=0.99,min_len = 10**6):\n",
    "        self.trajectories = trajectories\n",
    "        self.context_len = context_len\n",
    "        self.dev = dev\n",
    "        min_len = min(min_len, len(self.trajectories[0]['observations'])) ##len(data[0]['observations'])<self.args.context_len\n",
    "\n",
    "\n",
    "        \n",
    "        #states = []\n",
    "        #for traj in self.trajectories:\n",
    "        #    traj_len = traj['observations'].shape[0]\n",
    "        #    min_len = min(min_len, traj_len)\n",
    "        #    states.append(traj['observations'])\n",
    "        #    # calculate returns to go and rescale them \n",
    "\n",
    "        self.pad_init()\n",
    "        \n",
    "        self.trajectories[0]['returns_to_go_cal'] = discount_cumsum(self.trajectories[0]['rewards'], gamma) / rtg_scale\n",
    "\n",
    "        self.trajectories[0]['current_troops_count'] = (self.trajectories[0]['current_troops_count'] - 5.2496)/1.4733\n",
    "        \n",
    "        #print(min_len)\n",
    "        \n",
    "        # used for input normalization\n",
    "        \n",
    "        #states = torch.concatenate(states, axis=0).to(dtype = torch.float32)\n",
    "        #self.state_mean, self.state_std = torch.mean(self.trajectories[0]['observations'], axis=0\n",
    "        #                                            ), torch.std(self.trajectories[0]['observations'], axis=0) + 1e-6\n",
    "\n",
    "        # normalize states\n",
    "        #for traj in self.trajectories:\n",
    "\n",
    "            #self.trajectories[0]['current_troops_count']\n",
    "            #traj['observations'] = (traj['observations'].to(dtype=torch.float32) - self.state_mean) / self.state_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def pad_init(self):\n",
    "\n",
    "\n",
    "\n",
    "        #observations : torch.Size([255, 40]) torch.Size([2442, 40]) \n",
    "        # returntogo : torch.Size([255, 1]) torch.Size([2442, 1]) \n",
    "        # returntogo_pred : torch.Size([255, 1]) torch.Size([2442, 1]) \n",
    "        # rewards : torch.Size([255]) torch.Size([2442]) \n",
    "        # terminals : torch.Size([255]) torch.Size([2442]) \n",
    "        # actions :\n",
    "        #torch.Size([255, 2]) torch.Size([2442, 2]) \n",
    "        # action_masks : torch.Size([255, 32]) torch.Size([2442, 32]) \n",
    "        # current_agent_simple : torch.Size([255, 1]) torch.Size([2442, 1]) \n",
    "        # current_agent : torch.Size([255, 2]) torch.Size([2442, 2]) \n",
    "        # current_phase : torch.Size([255, 2]) torch.Size([2442, 2]) \n",
    "        # current_troops_count : torch.Size([255]) torch.Size([2442])\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        self.trajectories[0]['observations'] =  torch.cat( (\n",
    "                                              self.trajectories[0]['observations'][0].repeat([self.context_len -1]+[1 for i in range(len(self.trajectories[0]['observations'].shape)-1) ]),\n",
    "                                              self.trajectories[0]['observations']\n",
    "                                              \n",
    "                                            ),dim=0)          \n",
    "        \n",
    "\n",
    "        self.trajectories[0]['returntogo'] =      torch.cat( (\n",
    "                                              self.trajectories[0]['returntogo'][[0]].repeat([self.context_len -1]+[1 for i in range(len(self.trajectories[0]['returntogo'].shape)-1) ]),\n",
    "                                              self.trajectories[0]['returntogo']\n",
    "                                              \n",
    "                                            ),dim=0)         # torch.tensor([1,2,3,4])\n",
    "        self.trajectories[0]['returntogo_pred'] =             torch.cat( (\n",
    "                                              self.trajectories[0]['returntogo_pred'][[0]].repeat([self.context_len -1]+[1 for i in range(len(self.trajectories[0]['returntogo_pred'].shape)-1) ]),\n",
    "                                              self.trajectories[0]['returntogo_pred']\n",
    "                                              \n",
    "                                            ),dim=0)   # torch.tensor([1,2,3,4])\n",
    "\n",
    "        self.trajectories[0]['rewards'] =    torch.cat( (\n",
    "                                              torch.zeros([self.context_len -1]).to(device=self.dev),\n",
    "                                              self.trajectories[0]['rewards']\n",
    "                                              \n",
    "                                            ),dim=0)  # torch.tensor([1,2,3,4])\n",
    "        self.trajectories[0]['terminals'] =    torch.cat( (\n",
    "                                              self.trajectories[0]['terminals'][[0]].repeat([self.context_len -1]+[1 for i in range(len(self.trajectories[0]['terminals'].shape)-1) ]),\n",
    "                                              self.trajectories[0]['terminals']\n",
    "                                              \n",
    "                                            ),dim=0)  # torch.tensor([1,2,3,4])\n",
    "\n",
    "        \n",
    "        self.trajectories[0]['actions_1'] =      torch.cat( (\n",
    "                                              torch.zeros([self.context_len -1]+list(self.trajectories[0]['actions_1'].shape[1:])).to(device=self.dev),\n",
    "                                              self.trajectories[0]['actions_1']\n",
    "                                              \n",
    "                                            ),dim=0)\n",
    "        \n",
    "        self.trajectories[0]['actions_2'] =      torch.cat( (\n",
    "                                              torch.zeros([self.context_len -1]+list(self.trajectories[0]['actions_2'].shape[1:])).to(device=self.dev),\n",
    "                                              self.trajectories[0]['actions_2']\n",
    "                                              \n",
    "                                            ),dim=0)\n",
    "\n",
    "        self.trajectories[0]['log_probs_actions_2'] = torch.cat( (\n",
    "                                              torch.zeros([self.context_len -1]+list(self.trajectories[0]['log_probs_actions_2'].shape[1:])).to(device=self.dev),\n",
    "                                              self.trajectories[0]['log_probs_actions_2']\n",
    "                                              \n",
    "                                            ),dim=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # torch.tensor([1,2,3,4])\n",
    "        self.trajectories[0]['action_masks'] =    torch.cat( (\n",
    "                                              torch.zeros([self.context_len -1]+list(self.trajectories[0]['action_masks'].shape[1:])).to(device=self.dev,dtype=torch.float32),\n",
    "                                              self.trajectories[0]['action_masks']\n",
    "                                              \n",
    "                                            ),dim=0)\n",
    "\n",
    "\n",
    "\n",
    "        self.trajectories[0]['current_agent_acting'] =     torch.cat( (\n",
    "                                                              torch.zeros([self.context_len -1]+list(self.trajectories[0]['current_agent_acting'].shape[1:])).to(device=self.dev,dtype=torch.float32),\n",
    "                                                              self.trajectories[0]['current_agent_acting']\n",
    "                                                              \n",
    "                                                            ),dim=0)\n",
    "        \n",
    "        self.trajectories[0]['current_agent_simple'] =     torch.cat( (\n",
    "                                                              torch.zeros([self.context_len -1]+list(self.trajectories[0]['current_agent_simple'].shape[1:])).to(device=self.dev,dtype=torch.float32),\n",
    "                                                              self.trajectories[0]['current_agent_simple']\n",
    "                                                              \n",
    "                                                            ),dim=0)\n",
    "\n",
    "        \n",
    "        self.trajectories[0]['current_agent'] =             torch.cat( (\n",
    "                                                              torch.zeros([self.context_len -1]+list(self.trajectories[0]['current_agent'].shape[1:])).to(device=self.dev,dtype=torch.float32),\n",
    "                                                              self.trajectories[0]['current_agent']\n",
    "                                                              \n",
    "                                                            ),dim=0)\n",
    "\n",
    "        \n",
    "        self.trajectories[0]['current_phase'] =             torch.cat( (\n",
    "                                                          self.trajectories[0]['current_phase'][0].repeat([self.context_len -1]+[1 for i in range(len(self.trajectories[0]['current_phase'].shape)-1) ]),\n",
    "                                                          self.trajectories[0]['current_phase']\n",
    "                                                          \n",
    "                                                        ),dim=0) \n",
    "        self.trajectories[0]['current_troops_count'] =      torch.cat( (\n",
    "                                              self.trajectories[0]['current_troops_count'][0].repeat([self.context_len -1]),\n",
    "                                              self.trajectories[0]['current_troops_count']\n",
    "                                              \n",
    "                                            ),dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        #print(len(self.trajectories),(self.trajectories[0].shape),len(self.trajectories[0])- self.context_len + 1 )\n",
    "        return sum(max(0, len(traj['observations'])- self.context_len + 1\n",
    "                      ) for traj in self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        total_len = 0\n",
    "        for traj in self.trajectories:\n",
    "            \n",
    "            #print(total_len, idx - total_len, total_len + len(traj), - self.context_len + 1)\n",
    "            \n",
    "            if total_len  <= idx < total_len + len(traj['observations']) - self.context_len + 1    :\n",
    "                si = idx - total_len\n",
    "                \n",
    "                #context = traj[si:si + self.context_length]\n",
    "                states = (traj['observations'][si : si + self.context_len])\n",
    "                \n",
    "                actions_1 = traj['actions_1'][si : si + self.context_len]#torch.cat((traj['actions_1'][si : si + self.context_len-1].clone().detach(),   traj['actions_1'][[si + self.context_len-1]]      ),dim =0)\n",
    "                actions_2 = traj['actions_2'][si : si + self.context_len]#torch.cat((traj['actions_2'][si : si + self.context_len-1].clone().detach(),   traj['actions_2'][[si + self.context_len-1]]      ),dim =0)\n",
    "\n",
    "                log_probs_actions_2 = traj['log_probs_actions_2'][si : si + self.context_len]\n",
    "                \n",
    "                reward =  (traj['rewards'][si : si + self.context_len])\n",
    "                returntogo = (traj['returntogo'][si : si + self.context_len])\n",
    "                returns_to_go_cal = (traj['returns_to_go_cal'][si : si + self.context_len])\n",
    "                returntogo_pred = torch.cat((traj['returntogo_pred'][si : si + self.context_len-1].clone().detach(),  traj['returntogo_pred'][[ si + self.context_len-1]]       ),dim =0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                action_masks = (traj['action_masks'][si : si + self.context_len])\n",
    "\n",
    "                current_agent_acting = (traj['current_agent_acting'][si : si + self.context_len])\n",
    "                current_agent = (traj['current_agent'][si : si + self.context_len])\n",
    "                current_agent_simple = (traj['current_agent_simple'][si : si + self.context_len])\n",
    "                current_phase = (traj['current_phase'][si : si + self.context_len])\n",
    "                current_troops_count = (traj['current_troops_count'][si : si + self.context_len])\n",
    "    \n",
    "                \n",
    "                timesteps = torch.arange(start=si, end=si+self.context_len, step=1)\n",
    "    \n",
    "                # all ones since no padding\n",
    "                traj_mask = torch.ones(self.context_len, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "                if self.context_len> len(states):\n",
    "                    padding_len = self.context_len - len(states)\n",
    "    \n",
    "                    states                = torch.cat([states,\n",
    "                                    torch.zeros(([padding_len] + list(states.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)  \n",
    "                    actions_1               = torch.cat([actions_1,\n",
    "                                    torch.zeros(([padding_len] + list(actions_1.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "                    \n",
    "                    actions_2               = torch.cat([actions_2,\n",
    "                                    torch.zeros(([padding_len] + list(actions_2.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "\n",
    "                    log_probs_actions_2 =  torch.cat([log_probs_actions_2,\n",
    "                                    torch.zeros(([padding_len] + list(log_probs_actions_2.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "                    \n",
    "                    reward                = torch.cat([reward,\n",
    "                                    torch.zeros(([padding_len] + list(reward.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)  \n",
    "\n",
    "\n",
    "                    returntogo            = torch.cat([returntogo,\n",
    "                                    torch.zeros(([padding_len] + list(returntogo.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "                    returns_to_go_cal     = torch.cat([returns_to_go_cal,\n",
    "                                    torch.zeros(([padding_len] + list(returns_to_go_cal.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "                    returntogo_pred       = torch.cat([returntogo_pred,\n",
    "                                    torch.zeros(([padding_len] + list(returntogo_pred.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                    action_masks          = torch.cat([action_masks,\n",
    "                                    torch.zeros(([padding_len] + list(action_masks.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)        \n",
    "\n",
    "                    current_agent_acting = torch.cat([current_agent_acting,\n",
    "                                    torch.zeros(([padding_len] + list(current_agent_acting.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "                    \n",
    "                    current_agent         = torch.cat([current_agent,\n",
    "                                    torch.zeros(([padding_len] + list(current_agent.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "                    \n",
    "                    current_troops_count = torch.cat([current_troops_count,\n",
    "                                        torch.zeros(([padding_len] + list(current_troops_count.shape[1:])),\n",
    "                                        dtype=current_troops_count.dtype\n",
    "                                                   )], \n",
    "                                       dim=0)\n",
    "                    current_phase         = torch.cat([current_phase,\n",
    "                                    torch.zeros(([padding_len] + list(current_phase.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)         \n",
    "                    current_troops_count  = torch.cat([current_troops_count,\n",
    "                                    torch.zeros(([padding_len] + list(current_troops_count.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)                \n",
    "                    traj_mask             = torch.cat([traj_mask,\n",
    "                                    torch.zeros(([padding_len] + list(traj_mask.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                return  timesteps, states, actions_1,actions_2,log_probs_actions_2, returntogo, returns_to_go_cal, returntogo_pred,reward, traj_mask ,action_masks,current_agent_acting,current_agent_simple,current_agent,current_phase,current_troops_count\n",
    "                \n",
    "                #return pad(torch.tensor(context), (0,(self.context_length - len(context))),mode=\"constant\"), [1]\n",
    "\n",
    "            total_len += len(traj) - self.context_len + 1\n",
    "\n",
    "        raise IndexError(\"Index out of range 1\")\n",
    "\n",
    "\n",
    "class TrajectoryDataset_2_through_episodes(Dataset):\n",
    "    def __init__(self, trajectories):\n",
    "        self.trajectories = trajectories\n",
    "\n",
    "        #all_obs = torch.concat([ traj.dataset.trajectories[0]['observations'] for traj in self.trajectories],axis=0)\n",
    "        #self.state_mean = torch.mean(all_obs,axis =0)\n",
    "        #self.state_std = torch.std(all_obs,axis =0) + 1e-6\n",
    "        \n",
    "        #for traj in self.trajectories:\n",
    "        #    traj.dataset.trajectories[0]['observations'] = (traj.dataset.trajectories[0]['observations'].to(dtype=torch.float32) - self.state_mean) / self.state_std\n",
    "\n",
    "\n",
    "        #print(self.state_mean,self.state_std)\n",
    "        \n",
    "    #def get_state_stats(self):\n",
    "        #return self.state_mean, self.state_std        \n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        total_len = 0\n",
    "        if total_len  <= idx < total_len + len(self.trajectories)  :\n",
    "            return [batch for batch in self.trajectories[idx] ][0]\n",
    "\n",
    "\n",
    "        raise IndexError(\"Index out of range 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce951d8-af89-4bc1-8b2b-c1550a0d5977",
   "metadata": {},
   "source": [
    "# trainer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "023e851a-095e-4609-8e20-8fe69959bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ## Trainer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Args,param_dict =dict({})\n",
    "                 ):\n",
    "        # #### Configurations\n",
    "\n",
    "        self.args = Args()#tyro.cli(Args)\n",
    "        self.param_dict = param_dict\n",
    "        self.update_arg(param_dict=param_dict)\n",
    "        self.device = self.args.device#torch.device(\"cuda\" if torch.cuda.is_available() and self.args.cuda else \"cpu\")\n",
    "        \n",
    "        \n",
    "        #self.args.batch_size = int(self.args.num_envs * self.args.num_steps)\n",
    "        self.args.minibatch_size = int(self.args.batch_size // self.args.num_minibatches)\n",
    "        #self.args.num_iterations = self.args.total_timesteps // self.args.batch_size\n",
    "        self.gam = self.args.gamma\n",
    "        #self.args.minibatch_size = 256#128 \n",
    "        self.num_steps = self.args.num_steps#120000#1000000\n",
    "        self.num_iterations = self.args.num_iterations\n",
    "        self.episode_time_lim = self.args.episode_time_lim\n",
    "        self.hero_agent_count = self.args.hero_agent_count\n",
    "        self.env_config = self.args.env_config\n",
    "        self.num_episodes = self.args.num_episodes\n",
    "        self.context_len=self.args.model_config['context_len'] #200\n",
    "\n",
    "        self.training_performance_return = []\n",
    "        \n",
    "        #self.env_config = dict(render_mode = 'rgb_array', default_attack_all  = True,\n",
    "        #                    agent_count  = 4\n",
    "        #                       ,use_placement_perc=True,render_=False)        \n",
    "        \n",
    "        self.run_name = f\"{self.args.env_id}__{self.args.exp_name}__{self.args.seed}__{int(time.time())}\"\n",
    "\n",
    "        \n",
    "\n",
    "        TB_log = self.args.TB_log \n",
    "        if TB_log:    \n",
    "            self.writer = SummaryWriter(f\"runs/{self.run_name}\")\n",
    "            self.writer.add_text(\n",
    "                \"hyperparameters\",\n",
    "                \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(self.args).items()])),\n",
    "            )\n",
    "        else:\n",
    "            self.writer = None\n",
    "        \n",
    "        # TRY NOT TO MODIFY: seeding\n",
    "        random.seed(self.args.seed)\n",
    "        np.random.seed(self.args.seed)\n",
    "        #torch.manual_seed(self.args.seed)\n",
    "        \n",
    "        torch.backends.cudnn.deterministic = self.args.torch_deterministic\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.playe_r = 1#\"agent_1\" #\n",
    "        \n",
    "\n",
    "        \n",
    "        self.action_shape = (2,)\n",
    "\n",
    "\n",
    "\n",
    "        self.env = env_risk(**self.env_config)\n",
    "        \n",
    "        self.env.reset(seed=42)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        self.args.total_agents = self.total_agents  = len(self.env.possible_agents)\n",
    "        self.args.total_phases = self.total_phases = len(self.env.phases)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        print(torch.tensor(self.env.last()[0]['observation']\n",
    "                          ))\n",
    "\n",
    "        print(torch.tensor(self.env.last()[0]['observation']\n",
    "                          ).to(device=self.device))\n",
    "        sample_obs = self.obs_converter(torch.tensor(self.env.last()[0]['observation']\n",
    "                                                    ).to(device=self.device),\n",
    "                                        num_classes = self.total_agents+1\n",
    "                                       )\n",
    "        \n",
    "        self.ob_space_shape = sample_obs.shape #env.observation_space(playe_r)['observation'].shape\n",
    "        self.action_mask_shape = self.env.observation_space(self.playe_r)['action_mask'].shape\n",
    "        \n",
    "        #self.device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "        \n",
    "        \n",
    "        self.agent_list = list(self.env.possible_agents)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        self.the_hero_agent = 1\n",
    "\n",
    "        \n",
    "        self.qnet_config_dict = dict(action_space = self.env.action_space(self.playe_r\n",
    "                                                                         ).shape[0],\n",
    "                                    ob_space=(np.prod(self.ob_space_shape\n",
    "                                                    )+np.prod(self.action_mask_shape)\n",
    "                                         +1*( self.total_agents -1) #the current_agent +1#who actor agent was\n",
    "                                         +1*(self.total_phases -1)#the current phase\n",
    "                                         +1 )# the number of troops\n",
    "                               )\n",
    "        self.actor_config_dict =  dict(env=self.env,\n",
    "                        action_space = self.env.observation_space(self.playe_r)['action_mask'].shape[0],\n",
    "                        ob_space=(np.prod(self.ob_space_shape)\n",
    "                                         +np.prod(self.action_mask_shape)\n",
    "                                         +1*( self.total_agents-1) #the current_agent +1#who actor agent was\n",
    "                                         +1*(self.total_phases -1)#the current phase\n",
    "                                         +1) # the number of troops\n",
    "                               )\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #torch.Tensor(torch.hstack((observation['observation'].reshape(-1),\n",
    "        #                            torch.tensor(observation['action_mask'].reshape(-1)).to(self.device),\n",
    "        #                                   phase_mapping,\n",
    "        #                                    curr_agent_mapping,\n",
    "        #                                   torch.tensor([env.board.agents[curr_agent].bucket ]).to(self.device)))[None,:]#.repeat(3,axis = 0)\n",
    "        #                                        ).float()\n",
    "                        \n",
    "        \n",
    "        self.hero_agents_list = {i:Hero_agent(i) for i in range(1,self.hero_agent_count+1) } # this is a list , need to pass it as an argument\n",
    "        \n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_properties(self.total_agents,self.env.phases)        \n",
    "\n",
    "\n",
    "            print(len(self.args.model_name[i]))\n",
    "            \n",
    "            self.hero_agents_list[i].init_model(model_name=self.args.model_name[i], kwarg = dict(\n",
    "\n",
    "                                                qnet_config_dict = self.qnet_config_dict, \n",
    "                                                actor_config_dict = self.actor_config_dict,\n",
    "                                                args = self.args,\n",
    "                                                device = self.device,\n",
    "                                                writer=self.writer,\n",
    "                                                run_name=self.run_name,\n",
    "                                                agent=i)\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            #self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "            #self.qf1_target.load_state_dict(self.qf1.state_dict())\n",
    "            #self.q_optimizer = optim.Adam(list(self.qf1.parameters()), lr=self.args.learning_rate)\n",
    "            #self.actor_optimizer = optim.Adam(list(self.actor.parameters()), lr=self.args.learning_rate)\n",
    "\n",
    "    def update_arg(self,param_dict=dict({})):\n",
    "       for i,j in param_dict.items():\n",
    "           setattr(self.args,i,j)\n",
    "\n",
    "        \n",
    "\n",
    "    def obs_converter(self,  data, num_classes = 4, col =0 ):\n",
    "\n",
    "        if col != None:\n",
    "\n",
    "            #print(data.device)\n",
    "            #print(nn.functional.one_hot(data[:4,col].detach().long(), \n",
    "            #                                            num_classes = num_classes).to(self.device))\n",
    "            return torch.concat((nn.functional.one_hot(data[:,col].detach().long(), \n",
    "                                                        num_classes = num_classes).to(self.device),\n",
    "                                      data[:,~col,None]\n",
    "                                ),axis=1\n",
    "                               )[:,1:].to(self.device)\n",
    "    \n",
    "    def map_agent_phase_hot(self, data,num_classes = 3):\n",
    "        with torch.no_grad():\n",
    "            return nn.functional.one_hot(torch.tensor(data),num_classes = num_classes)[1:].to(self.device)\n",
    "    \n",
    "    def map_agent_phase_vector(self, data,num_classes = 3):\n",
    "        with torch.no_grad():\n",
    "            return nn.functional.one_hot(data[:,0].long(), \n",
    "                                                            num_classes = num_classes)[:,1:].to(self.device)\n",
    "\n",
    "\n",
    "    def train_loop_init(self):\n",
    "        self.gamma_t = {i:0 for i in self.env.possible_agents}\n",
    "        \n",
    "        \n",
    "        self.draw_count = 0\n",
    "\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_win_count_iter(self.total_agents )\n",
    "            self.hero_agents_list[i].model.init_path()\n",
    "        \n",
    "         \n",
    "        #self.first_count = 0\n",
    "        #self.second_count = 0\n",
    "        #self.third_count = 0\n",
    "        #self.third_count_draw = 0\n",
    "        \n",
    "        self.start_time = time.time()\n",
    "        self.global_step = 0\n",
    "        #self.faulting_player = \"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    def run_training_loop(self):\n",
    "        \"\"\"\n",
    "        ### Run training loop\n",
    "        \"\"\"\n",
    "\n",
    "        # last 100 episode information\n",
    "        #tracker.set_queue('reward', 100, True)\n",
    "        #tracker.set_queue('length', 100, True)\n",
    "\n",
    "\n",
    "        for i in self.hero_agents_list: # each agent has his own buffer, this is kinda pain because now this information is stored and not discarded\n",
    "    \n",
    "            self.hero_agents_list[i].rb = ReplayBuffer(\n",
    "                    self.args.buffer_size,\n",
    "                    Box(low =0, high=2000, shape =(self.qnet_config_dict['ob_space']+1,), dtype=np.float32),\n",
    "                    Box(low =0, high=2000, shape =(2,), dtype=np.float32),\n",
    "                    self.device,\n",
    "                    handle_timeout_termination=False,\n",
    "                )\n",
    "\n",
    "        env = env_risk(**(self.env_config #| {\"render_mode\" : None, \"bad_mov_penalization\" : 0.01,\"render_\":False}\n",
    "                         ))\n",
    "        env.reset(42)\n",
    "        \n",
    "        self.train_loop_init()\n",
    "        #self.paths=[]\n",
    "        \n",
    "        self.training_performance_return = []\n",
    "        \n",
    "        for iteration in range(1, self.num_iterations+1):\n",
    "\n",
    "\n",
    "            \n",
    "            self.sample(\n",
    "                                env,iteration,\n",
    "                                \n",
    "                                \n",
    "                 \n",
    "                            )\n",
    "\n",
    "            for i in self.hero_agents_list:\n",
    "                    self.hero_agents_list[i].model.create_training_dataset()\n",
    "\n",
    "            \n",
    "\n",
    "            #self.traj_dataset = D4RLTrajectoryDataset(self.paths, context_len=self.args.context_len,\n",
    "            #                                             rtg_scale=self.args.rtg_scale,gamma=self.args.gamma)\n",
    "            #                                            #rtg_scale=1,gamma=0.99\n",
    "    \n",
    "            #traj_dataset = traj_dataset.to(self.args.pin_memory_device)\n",
    "            \n",
    "            #self.traj_data_loader = DataLoader(self.traj_dataset,\n",
    "            #\t\t\t\t\t\tbatch_size=self.args.batch_size,\n",
    "            #                        shuffle= self.args.shuffle,\n",
    "            #                        pin_memory= self.args.pin_memory,\n",
    "            #                        drop_last = self.args.drop_last,\n",
    "            #                        pin_memory_device=self.args.pin_memory_device\n",
    "            #                        #shuffle=True,\n",
    "            #                        #pin_memory=True,\n",
    "            #                        #drop_last=True\n",
    "            #                        )\n",
    "\n",
    "            #self.traj_dataset = TrajectoryDataset_2_through_episodes(self.paths ) # a dataset of dataloaders\n",
    "            #self.traj_data_loader = DataLoader(self.traj_dataset, batch_size=1,shuffle=self.args.shuffle,\n",
    "            #                        pin_memory= self.args.pin_memory,\n",
    "            #                        drop_last = self.args.drop_last,\n",
    "            #                        pin_memory_device=self.args.pin_memory_device) # only spit 1 episode a time\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            #self.data_iter = iter(self.traj_data_loader)\n",
    "            \n",
    "            ## get state stats from dataset\n",
    "            #state_mean, state_std = self.traj_dataset.get_state_stats()\n",
    "\n",
    "            #print(len(self.paths))\n",
    "            #print(len(traj_dataset))\n",
    "            \n",
    "            #print(len(self.data_iter))\n",
    "            #print(next(data_iter))\n",
    "            #(timesteps, states, actions, returns_to_go,reward, traj_mask ,\n",
    "            # action_masks,current_agent,current_phase,current_troops_count) = next(self.data_iter)\n",
    "\n",
    "            #for batch in self.traj_data_loader\n",
    "\n",
    "            #print(states.shape)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            self.train(iteration)\n",
    "            #break\n",
    "\n",
    "            \n",
    "            \n",
    "            #if self.global_step%100 ==0:\n",
    "            #    SPS = int(self.global_step / (time.time() - self.start_time))\n",
    "            #    print(\"SPS:\", SPS)       \n",
    "            #    self.writer.add_scalar(\"charts/SPS\", SPS, self.global_step)\n",
    "        \n",
    "            \n",
    "            self.save_models()\n",
    "\n",
    "    def train(self,iteration):\n",
    "\n",
    "        if True:#self.global_step > self.args.learning_starts:\n",
    "\n",
    "            for i in self.hero_agents_list:\n",
    "                self.hero_agents_list[i].model.train_write(\n",
    "                        iteration,print_=True)\n",
    "                \n",
    "                        \n",
    "                    \n",
    "    \n",
    "    def train_(self,iteration):\n",
    "        \n",
    "        for epoch in range(self.args.update_epochs):\n",
    "            \n",
    "            if self.global_step > self.args.learning_starts:\n",
    "\n",
    "                for i in self.hero_agents_list:\n",
    "                    self.hero_agents_list[i].model.train_write(self.hero_agents_list[i].rb.sample(self.args.batch_size)\n",
    "                                                         ,iteration,epoch)\n",
    "                    \n",
    "    def save_models(self):\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].save_models()  \n",
    "\n",
    "    def reset_moves_hero_agents(self):\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_move_count_epi(self.env.phases)\n",
    "    \n",
    "    def sample(self,env,iteration\n",
    "                                ):\n",
    "        \n",
    "\n",
    "        #for i in self.hero_agents_list:\n",
    "        #    self.hero_agents_list[i].model.init_path()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        #if True:\n",
    "            # sample `worker_steps` from each worker\n",
    "            #there are no worker steps... rather there are full episodes\n",
    "\n",
    "            step = 0\n",
    "            fault_condition = False\n",
    "            clear_output(wait=True)\n",
    "            phase = 0\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            for episode in range(self.num_episodes):#num_episodes):\n",
    "                obs = torch.zeros((self.num_steps,) + self.ob_space_shape, requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "                \n",
    "                actions_1 = torch.zeros((self.num_steps,) ).to(self.device,    dtype = torch.float32)\n",
    "                \n",
    "                actions_2 = torch.zeros( (self.num_steps,)).to(self.device,    dtype = torch.float32)\n",
    "                log_probs_actions_2 = torch.zeros( (self.num_steps,)).to(self.device,    dtype = torch.float32)\n",
    "                \n",
    "                action_masks = torch.zeros((self.num_steps, ) + self.action_mask_shape, requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "                current_agent = torch.zeros((self.num_steps,1), requires_grad =False).to(self.device,    dtype = torch.float32)*0#-1\n",
    "                current_agent_acting = torch.ones((self.num_steps,1), requires_grad =False).to(self.device,    dtype = torch.float32)*0\n",
    "                current_phase = torch.zeros((self.num_steps,1), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "                current_troops_count = torch.zeros((self.num_steps,self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "                #logprobs = torch.zeros((self.num_steps, ), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "                rewards = torch.zeros((self.num_steps, self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "                rewards_2 = torch.zeros((self.num_steps, self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "                \n",
    "                returntogo = torch.zeros((self.num_steps, self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "                dones = torch.zeros((self.num_steps, self.total_agents), requires_grad =False).to(self.device)\n",
    "                dones_2 = torch.zeros((self.num_steps, self.total_agents), requires_grad =False).to(self.device)\n",
    "                #values = torch.zeros((self.num_steps,  )).to(self.device)\n",
    "                episodes = torch.ones((self.num_steps, ), requires_grad =False).to(self.device,    dtype = torch.float32)*-1\n",
    "                t_next = torch.zeros((self.num_steps, self.total_agents), requires_grad =False).to(self.device,    dtype = torch.float32)\n",
    "                    \n",
    "                total_rewards = {i:0 for i in env.possible_agents} #i can report this\n",
    "                #trace = tensor.zeros((self.context_len,self.qnet_config_dict['ob_space']))\n",
    "                action=1\n",
    "                #return2g = 110\n",
    "                \n",
    "                \n",
    "\n",
    "                for i in self.hero_agents_list:\n",
    "                    self.hero_agents_list[i].model.init_CL_sample_store()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "                if fault_condition:\n",
    "                    env = env_risk(**(self.env_config  #| {\"render_mode\" : None,\"bad_mov_penalization\" : 0.01,\"render_\":False#False\n",
    "                                                        # }\n",
    "                                     )\n",
    "                                      )#game.env(render_mode=None)\n",
    "\n",
    "                curren_epi = episode + (iteration-1)*self.num_episodes\n",
    "                env.reset(curren_epi) #for riplication\n",
    "                \n",
    "                fault_condition = False\n",
    "                step_count = 0\n",
    "                \n",
    "                self.reset_moves_hero_agents()\n",
    "                is_draw = 0\n",
    "                \n",
    "                #draw_territory_count = 0\n",
    "                #is_third = 0\n",
    "\n",
    "                for agent in env.agent_iter():\n",
    "                    e_t = env.terminations\n",
    "                    if sum(e_t.values()) <(self.total_agents-1):\n",
    "                        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "                        observation['observation'] =  self.obs_converter(\n",
    "                                                        torch.tensor(\n",
    "                                                            observation['observation']\n",
    "                                                        ).to(self.device,dtype=torch.float32),\n",
    "                                                        num_classes = self.total_agents+1)\n",
    "                        \n",
    "                        observation['observation'][:,-1]  =  (observation['observation'][:,-1] - 5.2496)/1.4733\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        episodes[step_count] = curren_epi\n",
    "                        obs[step_count] = observation['observation'] #torch.Tensor(observation['observation']).to(self.device) #sould i not add it .... meaning this is the last observation after the player dies\n",
    "                        action_masks[step_count] = torch.Tensor(observation['action_mask']).to(self.device)\n",
    "                        \n",
    "                        #curr_agent = agent#int(agent[-1])\n",
    "                        current_agent[step_count] = curr_agent = agent\n",
    "                        current_phase[step_count] = phase = env.phase_selection\n",
    "                        phase_mapping = self.map_agent_phase_hot(phase,num_classes = self.total_phases).float()\n",
    "                        \n",
    "                        curr_agent_mapping = self.map_agent_phase_hot(int(curr_agent)-1,\n",
    "                                                                      num_classes = self.total_agents \n",
    "                                                                     ).float()\n",
    "                        \n",
    "                        current_troops_count[step_count] = torch.tensor([env.board.agents[i].bucket for i in env.possible_agents],requires_grad =False).to(self.device)\n",
    "                    \n",
    "\n",
    "                        #model_in = torch.Tensor(torch.hstack((observation['observation'].reshape(-1),\n",
    "                        #                                      torch.tensor(observation['action_mask'].reshape(-1)).to(self.device)*(curr_agent == self.hero),\n",
    "                        #                   phase_mapping,\n",
    "                        #                   curr_agent_mapping,\n",
    "                        #                   torch.tensor([env.board.agents[ self.hero#curr_agent\n",
    "                        #                                                ].bucket ]).to(self.device)))[None,:]#.repeat(3,axis = 0)\n",
    "                        #                        ).float()\n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "                        for i in self.hero_agents_list:\n",
    "                            self.hero_agents_list[i].model.current_model_in(observation,curr_agent,\n",
    "                                                                            phase_mapping,curr_agent_mapping,\n",
    "                                                                            env_board_agents=env.board.agents)\n",
    "                            self.hero_agents_list[i].model.update_CL_sample_store(curr_agent_=i,\n",
    "                                                                                  inp = {'step':step_count,\n",
    "                                                                                         'act_2_1':[] ,\n",
    "                                                                                         'act_2_2':[] ,\n",
    "                                                                                         'curr_reward_list':[]\n",
    "                                          },before_action=1)\n",
    "                            \n",
    "                        \n",
    "                        #if e_t[curr_agent]:\n",
    "                            #print('heeee')\n",
    "                        action_taken = False\n",
    "                        log_prob_a2 = None  \n",
    "                        if True:\n",
    "                            \n",
    "                            mask = observation[\"action_mask\"]\n",
    "                            if (self.global_step < self.args.learning_starts) or (\n",
    "                                np.random.rand() > min(\n",
    "                                                ((curren_epi)/((self.num_iterations*self.num_episodes)/10))\n",
    "                                                , 0.95)\n",
    "                                                #) or (agent != self.the_hero_agent) \n",
    "                                                ) or ( agent not in self.hero_agents_list):\n",
    "        \n",
    "                                \n",
    "                                action = env.action_space(agent).sample()\n",
    "                                \n",
    "                                #part_0 =np.random.choice(np.where(env.board.calculated_action_mask(agent))[0])\n",
    "                                part_0 =np.random.choice(np.where(observation['action_mask'])[0])\n",
    "                                action = torch.tensor([\n",
    "                                                        [\n",
    "                                                         [part_0],\n",
    "                                                         [np.around(action[1],2)]\n",
    "                                                        ]\n",
    "                                                        ],requires_grad =False).to(self.device)\n",
    "                                \n",
    "                                action = action[:,:,0]\n",
    "                                action_1 = action[:,0]\n",
    "                                action_2 = action[:,1]\n",
    "                                log_prob_a2 = torch.tensor([  1/(sum(mask)+0.0001) ],requires_grad =False).to(self.device)\n",
    "                                for i in self.hero_agents_list:\n",
    "                                    \n",
    "                                    self.hero_agents_list[i].model.update_CL_sample_store(curr_agent_=curr_agent,\n",
    "                                                                                          inp = {'step':step_count,\n",
    "                                                                                                 'act_2_1':action_1 ,\n",
    "                                                                                                 'act_2_2':[] ,\n",
    "                                                                                                 'curr_reward_list':[]\n",
    "                                                  },before_action=2)\n",
    "                                    \n",
    "                                    self.hero_agents_list[i].model.update_CL_sample_store(curr_agent_=curr_agent,\n",
    "                                                                                          inp = {'step':step_count,\n",
    "                                                                                                 'act_2_1':[] ,\n",
    "                                                                                                 'act_2_2':action_2 ,\n",
    "                                                                                                 'curr_reward_list':[]\n",
    "                                                  },before_action=3)\n",
    "\n",
    "                                \n",
    "\n",
    "\n",
    "                            \n",
    "\n",
    "                                                              \n",
    "                            else:\n",
    "                                action_taken = True\n",
    "                                #action_masks[step]\n",
    "\n",
    "                                #need to update this\n",
    "                                action_1,_,action_taken,_ = self.hero_agents_list[curr_agent].action_predict(save_R=False,action_masks=action_masks[step_count])\n",
    "                                #action_1 = action_1[None,:]\n",
    "                                #so the action_1 should be updated here... and prediction of action_2 would be re-done\n",
    "\n",
    "\n",
    "                                \n",
    "                                \n",
    "                                if action_taken:\n",
    "                                    current_agent_acting[step_count] = curr_agent\n",
    "\n",
    "                                for i in self.hero_agents_list:\n",
    "                                    \n",
    "                                    self.hero_agents_list[i].model.update_CL_sample_store(curr_agent_=curr_agent,\n",
    "                                                                                          inp = {'step':step_count,\n",
    "                                                                                                 'act_2_1':action_1 ,\n",
    "                                                                                                 #'act_2_1':action_1[0] ,\n",
    "                                                                                                 'act_2_2':[] ,\n",
    "                                                                                                 'curr_reward_list':[]\n",
    "                                                  },before_action=2)\n",
    "\n",
    "                                _,action_2,_,log_prob_a2 = self.hero_agents_list[curr_agent].action_predict(save_R=False,action_masks=action_masks[step_count],return_log_prob_a2 = True)\n",
    "                               # action_2 = action_2[None,:]\n",
    "\n",
    "\n",
    "                                for i in self.hero_agents_list:\n",
    "                                    \n",
    "                                    self.hero_agents_list[i].model.update_CL_sample_store(curr_agent_=curr_agent,\n",
    "                                                                                          inp = {'step':step_count,\n",
    "                                                                                                 'act_2_1':[] ,\n",
    "                                                                                                 #'act_2_2':action_2[0] ,\n",
    "                                                                                                 'act_2_2':action_2 ,\n",
    "                                                                                                 'curr_reward_list':[]\n",
    "                                                  },before_action=3)\n",
    "                                    \n",
    "\n",
    "\n",
    "\n",
    "                            \n",
    "                                #print(action,action.requires_grad)\n",
    "                                #if len(action.shape)<2:\n",
    "                                    #print(episode,\"---2--\",action, action.shape)\n",
    "                                    #a()\n",
    "                                    #break\n",
    "                                \n",
    "                            \n",
    "                                #action = self.actor(torch.Tensor(model_in).to(self.device))\n",
    "\n",
    "                            #this is only to predict and save the return to go for all the transformer agents, will optimize later\n",
    "                            _ =  [ self.hero_agents_list[i].action_predict(save_R=True, action_masks=action_masks[step_count] # it only matters for the correct agent ... we are only saving the Q\n",
    "\n",
    "                                                                           \n",
    "                                                                          \n",
    "                                                                          ) for i in self.hero_agents_list]\n",
    "\n",
    "                            \n",
    "                            actions_1[step_count] = action_1\n",
    "                            actions_2[step_count] = action_2\n",
    "                            if log_prob_a2 != None:\n",
    "                                log_probs_actions_2[step_count] = log_prob_a2\n",
    "                            curr_agent_ = int(curr_agent)\n",
    "                            \n",
    "                            \n",
    "        \n",
    "                            if not observation['action_mask'][action_1.long()]: \n",
    "                                fault_condition =True\n",
    "                                \n",
    "                                #self.faulting_player = agent\n",
    "\n",
    "                                \n",
    "\n",
    "\n",
    "                                if  curr_agent_ in self.hero_agents_list:\n",
    "                                    self.hero_agents_list[curr_agent_].bad_move_count+=1\n",
    "                                    self.hero_agents_list[curr_agent_].bad_move_phase_count[int(current_phase[step_count][0])]+=1  # when is the where_is_it_performing_bad_really\n",
    "                                    #print('here',agent, action, observation['action_mask'])\n",
    "                            \n",
    "        \n",
    "                            if  curr_agent_ in self.hero_agents_list:\n",
    "                                self.hero_agents_list[curr_agent_].move_count[int(current_phase[step_count][0])]+=1  \n",
    "                            #if self.the_hero_agent == curr_agent:\n",
    "                                #move_count[int(current_phase[step][0])]+=1        \n",
    "        \n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            \n",
    "                        #print('here',agent, action)\n",
    "                        if action_1 != None :\n",
    "                            act_2_1 = action_1[0]\n",
    "                            act_2_2 = action_2[0]\n",
    "                            \n",
    "                            #act_2 = action.detach().cpu().numpy()[0]#list([action.detach().cpu().numpy()[0][0], max(action.detach().cpu().numpy()[0][1],0.1) ])\n",
    "                            #act_2 = torch.tensor([act_2[0], max(act_2[1],0.001) ]).to(device = self.device)\n",
    "                            #print(max(act_2_2.clone().detach().cpu().item(),0.001))\n",
    "                            env.step([act_2_1.clone().detach().cpu().item(), max(act_2_2.clone().detach().cpu().item(),0.001) ])  \n",
    "                            try:\n",
    "                                _ =1\n",
    "                                #env.step([act_2_1.clone().detach().cpu().item(), max(act_2_2.clone().detach().cpu().item(),0.001) ])    \n",
    "                            except Exception as e:\n",
    "                                print(\"action_taken\",action_taken)\n",
    "                                print(e)\n",
    "                                \n",
    "                                print([act_2_1.clone().detach().cpu(), max(act_2_2.clone().detach().cpu(),0.001) ])\n",
    "                                \n",
    "                            \n",
    "                        #env.step(act_2 if action != None else None)        \n",
    "        \n",
    "        \n",
    "                        if True:\n",
    "        \n",
    "                            \n",
    "                            curr_reward_list =  env.curr_rewards\n",
    "                            \n",
    "                            if (step_count == (self.episode_time_lim-1))   or (self.global_step == (self.num_steps-1)): # draw reward\n",
    "                                is_draw=1\n",
    "                                curr_reward_list = {i:-100 for i in env.possible_agents }\n",
    "\n",
    "                            \n",
    "\n",
    "                            #if self.hero == curr_agent_:\n",
    "                            #    DT_input['action'][-1]  =act_2\n",
    "                            #DT_input['return_to_go'][-1] -=  curr_reward_list[self.hero]\n",
    "                            #returntogo[step] = DT_input['return_to_go'][-1]\n",
    "\n",
    "                            for i in self.hero_agents_list:\n",
    "                                self.hero_agents_list[i].model.update_CL_sample_store(curr_agent_=curr_agent,\n",
    "                                              inp = {'step':step_count,\n",
    "                                                     'act_2_1':None ,\n",
    "                                                     'act_2_2':None ,\n",
    "                                                     'curr_reward_list':curr_reward_list[i]\n",
    "                                              },before_action=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            \n",
    "                            rewards_2[step_count] = torch.tensor([curr_reward_list[i] for i in env.possible_agents]).to(self.device,dtype=torch.float32)\n",
    "                            if step >1:\n",
    "                                dones_2[step_count] = torch.tensor([ int(env.terminations[i]) - dones_2[step_count-1,i-1]  for i in env.possible_agents]).to(self.device)\n",
    "                            else:\n",
    "                                dones_2[step_count] = torch.tensor([env.terminations[i] for i in env.possible_agents]).to(self.device)\n",
    "\n",
    "\n",
    "                        #list_curr_reward_list = np.array(list(curr_reward_list.values()))\n",
    "                        \n",
    "                        #if sum(curr_reward_list.values()) == -300:\n",
    "                            #print('here')\n",
    "                            #is_draw=1\n",
    "        \n",
    "                        \n",
    "                        for age_i in env.possible_agents:\n",
    "                            \n",
    "                            total_rewards[age_i]+=curr_reward_list[age_i] #env.curr_rewards[age_i] if (step_count != episode_time_lim) else -100\n",
    "                                    \n",
    "                        \n",
    "                        step +=1\n",
    "                        self.global_step+=1\n",
    "        \n",
    "                    else:\n",
    "                        self.training_performance_return.append(total_rewards[1])\n",
    "                        print('done:',env.terminations,#env.terminations.values(),\n",
    "                              \",total_reward:\",total_rewards, ',iteration:',iteration,\",episode:\", episode )\n",
    "                        print(env.board.territories)\n",
    "                        break    \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "                    step_count+=1\n",
    "                    \n",
    "                    if (self.global_step == self.num_steps) :# or (fault_condition and (fa ulting_player != agent) and (len(env.agents)==0)):\n",
    "                        \n",
    "                        print('global_break_1')\n",
    "                        self.training_performance_return.append(total_rewards[1])\n",
    "                        print('done:',env.terminations,#env.terminations.values(),\n",
    "                              \",total_reward:\",total_rewards, ',iteration:',iteration,\",episode:\", episode )\n",
    "                        print(env.board.territories)\n",
    "\n",
    "                        #have to get out of the outer loop\n",
    "                        break\n",
    "                    elif (step_count == self.episode_time_lim):\n",
    "                        print('episode_break_1')\n",
    "                        self.training_performance_return.append(total_rewards[1])\n",
    "                        print('done:',env.terminations,#env.terminations.values(),\n",
    "                              \",total_reward:\",total_rewards, ',iteration:',iteration,\",episode:\", episode )\n",
    "                        print(env.board.territories)\n",
    "                        break\n",
    "\n",
    "                #self.obs=obs\n",
    "                \n",
    "                #print_here\n",
    "                print(episode,step_count,iteration)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                #print(rewards[step-2])\n",
    "                if self.global_step == self.num_steps:\n",
    "                    print('global_break_2')\n",
    "                    break \n",
    "\n",
    "                for i in self.hero_agents_list:\n",
    "                    self.hero_agents_list[i].position =self.total_agents\n",
    "                    \n",
    "                    \n",
    "                #[ position = 3 for i in ] \n",
    "                for k_,(i_,j_) in enumerate(sorted([(j_,i_) for i_,j_ in total_rewards.items()],reverse=True) \n",
    "                      ):\n",
    "                    if int(j_) in self.hero_agents_list:\n",
    "                        self.hero_agents_list[int(j_)].position = k_+1\n",
    "                        print(j_,self.hero_agents_list[int(j_)].position)\n",
    "                        \n",
    "                        \n",
    "                    #if j_==self.the_hero_agent:\n",
    "                    #    position = k_+1\n",
    "\n",
    "                cur_epi_list = (episodes == curren_epi)\n",
    "                        \n",
    "                if self.args.TB_log:\n",
    "                    self.write_exploring(is_draw,#position,\n",
    "                            curren_epi,step,\n",
    "                            step_count,\n",
    "                            total_rewards,#bad_move_count\n",
    "                            #,bad_move_phase_count,\n",
    "                            #move_count,\n",
    "                            observation,\n",
    "                            env,\n",
    "                            cur_epi_list,\n",
    "                            current_agent_acting)\n",
    "\n",
    "                #paths = []\n",
    "\n",
    "                for i in self.hero_agents_list:\n",
    "                    self.hero_agents_list[i].model.update_train_data(\n",
    "                         step_count,\n",
    "                         obs,\n",
    "                            self.ob_space_shape,\n",
    "                            rewards_2[:,i-1],\n",
    "                            dones_2[:,i-1],\n",
    "                            actions_1[cur_epi_list],actions_2,log_probs_actions_2,\n",
    "                            action_masks,\n",
    "                            current_agent,\n",
    "                            current_agent_acting,\n",
    "                            current_phase,\n",
    "                            current_troops_count[:,i-1],\n",
    "                            map_agent_phase_vector = self.map_agent_phase_vector,\n",
    "                            \n",
    "                         )\n",
    "\n",
    "\n",
    "\n",
    "        #this  ---------------\n",
    "        #avg_episode_length = torch.mean(torch.tensor(\n",
    "        #                    [(episodes[:step] == i_epi).sum() for i_epi in episodes[:step].unique()]).float())#np.mean([(episodes[:step] == i_epi).sum() for i_epi in episodes[:step].unique()])\n",
    "        #if self.args.TB_log:\n",
    "        #    self.writer.add_scalar(\"charts/avg_episodic_length\", avg_episode_length, self.global_step)\n",
    "\n",
    "\n",
    "\n",
    "        #return paths\n",
    "        #return rb\n",
    "\n",
    "    def update_hero_rb(self,b_obs_a_all,t_next,t_range,current_agent,actions,\n",
    "                                rewards,dones,infos,current_troops_count,\n",
    "                                step):\n",
    "\n",
    "\n",
    "\n",
    "            for agent in self.hero_agents_list:\n",
    "                #the_hero_agent = agent\n",
    "                \n",
    "                \n",
    "                hero_steps = [current_agent == agent][0][:,0][:step]\n",
    "                next_indecies = (t_next[:step,agent-1].to(dtype=torch.int) + t_range +1)[hero_steps].long()\n",
    "                selected_t_next = t_next[:,agent-1,None]#t_next[:step,0,None][hero_steps]\n",
    "\n",
    "                b_obs_a = torch.concat((b_obs_a_all,\n",
    "                                        current_troops_count[:,agent-1,None],\n",
    "                                        selected_t_next),axis =1)\n",
    "                \n",
    "                for i in zip(b_obs_a[:step][hero_steps].cpu().to(dtype=torch.float), \n",
    "                             b_obs_a[next_indecies].cpu().to(dtype=torch.float), \n",
    "                            actions[:step][hero_steps].cpu().to(dtype=torch.float32),\n",
    "                             rewards[:step][hero_steps][:,agent-1,None].cpu(), \n",
    "                           dones[:step][hero_steps][:,agent-1,None].cpu(), infos):\n",
    "                            self.hero_agents_list[agent].rb.add(*i)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def write_exploring(self,is_draw,#position,\n",
    "                        curren_epi,step,\n",
    "                        step_count,\n",
    "                        total_rewards,#bad_move_count\n",
    "                        #,bad_move_phase_count,\n",
    "                        #move_count,\n",
    "                        observation,\n",
    "                        env,\n",
    "                        cur_epi_list,\n",
    "                        current_agent_acting):\n",
    "\n",
    "        if is_draw:\n",
    "            self.draw_count +=1\n",
    "\n",
    "            for i in self.hero_agents_list:\n",
    "\n",
    "                self.writer.add_scalar(f\"draw_charts_agent_{i}/position_draw\",self.hero_agents_list[i].position\n",
    "                                                                                            ,self.draw_count) #draw_count is the number of draw episodes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                self.hero_agents_list[i].draw_territory_count = int(observation['observation'][:,i].sum()) #this is the total number of states\n",
    "                \n",
    "                self.hero_agents_list[i].count_draw_dict[\n",
    "                                                        self.hero_agents_list[i].position\n",
    "                                                        ] +=1\n",
    "                self.writer.add_scalar(f\"draw_charts_agent_{i}/draw_territory_count\",\n",
    "                                                                               self.hero_agents_list[i].draw_territory_count,\n",
    "                                                                               self.draw_count)#self.global_step)\n",
    "                \n",
    "                for j in self.hero_agents_list[i].count_draw_dict:\n",
    "                    self.writer.add_scalar(f\"draw_charts_agent_{i}/{j}_position_prop_draw\",int(\n",
    "                                                                                            self.hero_agents_list[i].position==j\n",
    "                                                                                            ),self.draw_count)\n",
    "\n",
    "                    if j not in [1,2]:\n",
    "                        self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_all_prop\",int(\n",
    "                                                                                            self.hero_agents_list[i].position==j\n",
    "                                                                                            ),(curren_epi+1))\n",
    "\n",
    "                        self.writer.add_scalar(f\"draw_charts_agent_{i}/{j}_place_in_draw\",\n",
    "                                                                   self.hero_agents_list[i].count_draw_dict[j],\n",
    "                                                                   self.draw_count)\n",
    "\n",
    "                        self.writer.add_scalar(f\"draw_charts_agent_{i}/{j}_place_in_draw_ratio\",\n",
    "                                                                   self.hero_agents_list[i].count_draw_dict[j]/self.draw_count,\n",
    "                                                                   self.draw_count)\n",
    "                        \n",
    "                    \n",
    "            self.writer.add_scalar(\"draw_charts/draw_count\",self.draw_count,(curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(\"draw_charts/draw\",1,(curren_epi+1))\n",
    "            self.writer.add_scalar(\"draw_charts/draw_to_total_count\",self.draw_count/(curren_epi +1+0.000001),(curren_epi +1))#self.global_step)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            non_draw_count =(curren_epi-self.draw_count+1+0.000001)\n",
    "            for i in self.hero_agents_list:\n",
    "\n",
    "                self.writer.add_scalar(f\"win_charts_agent_{i}/position_win\",self.hero_agents_list[i].position\n",
    "                                                                                            ,(curren_epi+1))\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                self.hero_agents_list[i].count_dict[self.hero_agents_list[i].position\n",
    "                                               ] +=1\n",
    "\n",
    "                \n",
    "                for j in self.hero_agents_list[i].count_dict:\n",
    "                    self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_prop\",int(\n",
    "                                                            self.hero_agents_list[i].position==j\n",
    "                                                            ),(curren_epi+1))\n",
    "\n",
    "                    self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position\",self.hero_agents_list[i].count_dict[j],\n",
    "                                           (curren_epi+1))\n",
    "\n",
    "                    self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_to_total_terminated\",self.hero_agents_list[i].count_dict[j]/non_draw_count,(curren_epi+1))\n",
    "\n",
    "                    if j not in [1,2]:\n",
    "                        self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_all_prop\",int(\n",
    "                                                                                            self.hero_agents_list[i].position==j\n",
    "                                                                                            ),(curren_epi+1))\n",
    "\n",
    "\n",
    "            self.writer.add_scalar(\"draw_charts/draw\",0,(curren_epi+1))\n",
    "            \n",
    "        for i in self.hero_agents_list:\n",
    "\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/model_move_count_per_episode\",   sum(current_agent_acting[:step_count] ==i)  ,  (curren_epi +1))\n",
    "\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/model_2_total_move_count_per_episode\",   sum(current_agent_acting[:step_count] ==i)/sum( self.hero_agents_list[i].move_count.values()),  (curren_epi +1))\n",
    "\n",
    "            self.writer.add_scalar(f\"win_charts_agent_{i}/position_all\",self.hero_agents_list[i].position\n",
    "                                                                                            ,(curren_epi+1))\n",
    "\n",
    "            \n",
    "            for j in self.hero_agents_list[i].count_dict:\n",
    "                self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_to_total\",(\n",
    "                                                            self.hero_agents_list[i].count_dict[j]+\n",
    "                                                            self.hero_agents_list[i].count_draw_dict[j]\n",
    "                                                        \n",
    "                                                            )/(curren_epi +1+0.00001 ),(curren_epi+1))#global_step)\n",
    "\n",
    "            \n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_count,            (curren_epi +1))#self.global_step)\n",
    "                \n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_position_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[0],            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_attack_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[1],            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_fortify_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[2],            (curren_epi +1))#self.global_step)\n",
    "            \n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/total_moves\",sum(\n",
    "                                                    self.hero_agents_list[i].move_count.values()),            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_count_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_count/(sum(\n",
    "                                                   self.hero_agents_list[i].move_count.values())+1),            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_position_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[0]/( \n",
    "                                                   self.hero_agents_list[i].move_count[0]+1),            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_attack_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[1]/( \n",
    "                                                   self.hero_agents_list[i].move_count[1]+1),            (curren_epi +1))#self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_fortify_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[2]/( \n",
    "                                                   self.hero_agents_list[i].move_count[2]+1),            (curren_epi +1))#self.global_step)\n",
    "\n",
    "        \n",
    "        self.writer.add_scalar(\"charts/epsilon\",(curren_epi/((self.num_iterations*self.num_episodes)/10)),(curren_epi +1))#self.global_step)\n",
    "        self.writer.add_scalar(\"charts/avg_per_epi_total_reward\", np.mean(list(total_rewards.values())), (curren_epi +1))#self.global_step)\n",
    "\n",
    "        \n",
    "\n",
    "        #values_total = {i:0 for i in self.env.possible_agents}\n",
    "        \n",
    "\n",
    "        \n",
    "        self.writer.add_scalar(\"charts/episodic_length\", cur_epi_list[:step].sum(), (curren_epi +1))#self.global_step)\n",
    "        \n",
    "        for i in env.possible_agents:\n",
    "            #cur_index = torch.where((current_agent[:,0] == i) &( cur_epi_list ))[0]\n",
    "\n",
    "            #values_total[i] = values[cur_index].mean()\n",
    "            #writer.add_scalar(\"charts/mean_value_per_epi_agent_\"+str(i), values_total[i], global_step)\n",
    "            \n",
    "            self.writer.add_scalar(\"charts/total_reward_per_epi_agent_\"+str(i), total_rewards[i], (curren_epi +1))#self.global_step)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "581b1f43-c596-46d7-b65e-95bc9cff09de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35a5cb1c-f454-4687-a4ae-81386a5c31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, gamma,Torch = True):\n",
    "    if Torch:\n",
    "        disc_cumsum = torch.zeros_like(x,dtype = torch.float32)\n",
    "    else:\n",
    "        disc_cumsum = np.zeros_like(x,dtype = np.float32)\n",
    "    \n",
    "    disc_cumsum[-1] = x[-1]\n",
    "    #print(disc_cumsum[-1])\n",
    "    for t in reversed(range(x.shape[0]-1)):\n",
    "        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t+1]\n",
    "        #print(x[t],disc_cumsum[t])\n",
    "    return disc_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df9b3037-7bf1-432c-a58a-92ccd7af5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp_3 = dict(\n",
    "exp_name = 'exp3_ddqn_lr_bs_1',\n",
    "learning_rate = 0.001,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 2,\n",
    "entropy=False,\n",
    "return_prob=False,\n",
    "small = False,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "    \n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 4\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.01\n",
    "                 \n",
    "                     #| {\"render_mode\" : None,\"bad_mov_penalization\" : 0.01,\"render_\":False#False\n",
    "                     #                                   }\n",
    "                 \n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "exp_4 = dict(\n",
    "exp_name = 'exp4_ddqn_2_agents_1_hero',\n",
    "learning_rate = 0.001,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 1,\n",
    "entropy=False,\n",
    "return_prob=False,\n",
    "small = False,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 2\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.01\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "exp_5 = dict(\n",
    "exp_name = 'exp5_ddqn_2_agents_1_hero',\n",
    "learning_rate = 0.001,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 1,\n",
    "entropy=False,\n",
    "return_prob=False,\n",
    "small = False,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 2\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.1\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "exp_6 = dict(\n",
    "exp_name = 'exp6_ddqn_2_agents_1_hero',\n",
    "learning_rate = 0.0003,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 1,\n",
    "entropy=False,\n",
    "return_prob=False,\n",
    "small = False,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 2\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.1\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "exp_7 = dict(\n",
    "exp_name = 'exp7_ddqn_2_agents_1_hero',\n",
    "learning_rate = 0.0003,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 1,\n",
    "entropy=True,\n",
    "return_prob=2,\n",
    "actor_wt = 0.5,\n",
    "CE_wt = 0.01,\n",
    "small = False,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 2\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.1\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "exp_8 = dict(\n",
    "exp_name = 'exp8_ddqn_2_agents_1_hero_norm',\n",
    "learning_rate = 0.0003,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 1,\n",
    "entropy=True,\n",
    "return_prob=2,\n",
    "actor_wt = 0.5,\n",
    "CE_wt = 0.01,\n",
    "small = False,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 2\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.1\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "exp_9 = dict(\n",
    "exp_name = 'exp9_ddqn_4_agents_2_hero_norm',\n",
    "learning_rate = 0.0003,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 2,\n",
    "entropy=True,\n",
    "return_prob=2,\n",
    "actor_wt = 0.5,\n",
    "CE_wt = 0.01,\n",
    "small = False,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 4\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.1\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exp_10 = dict(\n",
    "exp_name = 'exp10_ddqn_4_agents_2_hero_norm_small',\n",
    "learning_rate = 0.0003,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 2,\n",
    "entropy=True,\n",
    "return_prob=2,\n",
    "actor_wt = 0.5,\n",
    "CE_wt = 0.01,\n",
    "small = True,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 4\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.01\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "exp_11 = dict(\n",
    "exp_name = 'exp11_ddqn_4_agents_2_hero_norm_small',\n",
    "learning_rate = 0.0003,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 2,\n",
    "entropy=True,\n",
    "return_prob=2,\n",
    "actor_wt = 0.5,\n",
    "CE_wt = 0.01,\n",
    "small = True,\n",
    "\n",
    "context_len = 64,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 5,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 4\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.01\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "exp_12 = dict(\n",
    "exp_name = 'exp12_ddqn_4_agents_2_hero_norm_small',\n",
    "learning_rate = 5e-4,#0.003,#0.0003,\n",
    "batch_size = 2,\n",
    "gamma = 0.99,\n",
    "num_steps=1200000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 3000,#10000,\n",
    "hero_agent_count = 1,\n",
    "model_name={1:\"transformer_model\"}#,2:'transformer_model'}\n",
    ",entropy=True,\n",
    "return_prob=2,\n",
    "actor_wt = 0.5,\n",
    "CE_wt = 0.01,\n",
    "small = True,\n",
    "num_episodes = 4,\n",
    "context_len = 256,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=False,#False,\n",
    "drop_last=True,\n",
    "TB_log=True,\n",
    "learning_starts =100,\n",
    "update_epochs = 1,\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "pin_memory_device= (\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "    \n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 3#4\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.01\n",
    "                 )\n",
    ",model_config = dict(\n",
    "                    n_blocks      =   3,\n",
    "                    embed_dim     =   64,#128 ,\n",
    "                    context_len   =   256,#256  ,\n",
    "                    n_heads       =   1,\n",
    "                    dropout_p     =   0.1,\n",
    "                    wt_decay      =   0.0001,\n",
    "                    warmup_steps  =   100   ,\n",
    "                    tau           =   0.95,\n",
    "                    chunk_size    =   64,#8,#32,#64\n",
    "                    chunk_overlap =   1,\n",
    "                    rb_len        =   20,\n",
    "                \n",
    "                    warmup_epoch  =   5,  \n",
    "                    total_epoch   =   20,\n",
    "                    initial_lr    =   5e-4,\n",
    "                    final_lr      =   1e-6,\n",
    "\n",
    "                    beta           = 0.5,#args.model_config['beta']         #0.2 #Q_mse\n",
    "                    alpha          = 1,#args.model_config['alpha']          #0.1  #actionloss\n",
    "                    entropy_coeff  = 0.2,#args.model_config['entropy_coeff']         #0.1#0.5   #entropy loss in action\n",
    "                    val_loss_coeff = 0.5#args.model_config['val_loss_coeff']        #0.5      #Q loss\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "                    )\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08cb290-680b-4c28-b496-62e3ee380ec3",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "506c53ba-1951-48b4-9474-55bf0ec85797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]], dtype=torch.int8)\n",
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]], device='cuda:0', dtype=torch.int8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31721\\AppData\\Local\\Temp\\5\\ipykernel_56224\\2221868122.py:84: UserWarning: expandable_segments not supported on this platform (Triggered internally at ..\\c10/cuda/CUDAAllocatorConfig.h:30.)\n",
      "  print(torch.tensor(self.env.last()[0]['observation']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "<class 'type'>\n"
     ]
    }
   ],
   "source": [
    "#T = Trainer(Args,param_dict = exp_3)\n",
    "T = Trainer(Args,param_dict = exp_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3387511-0428-4bcc-a3ef-875abd09c55d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: {1: True, 2: True, 3: True} ,total_reward: {1: -200.26999999999998, 2: -99.08, 3: 109.88} ,iteration: 305 ,episode: 0\n",
      "[[3 3.0]\n",
      " [3 4.0]\n",
      " [3 1.0]\n",
      " [0 0.0]\n",
      " [3 3.0]\n",
      " [3 1.0]\n",
      " [3 12.0]\n",
      " [3 1.0]\n",
      " [3 8.0]\n",
      " [3 1.0]]\n",
      "0 817 305\n",
      "1 3\n",
      "done: {1: True, 2: True, 3: True} ,total_reward: {1: -98.2, 2: 107.99, 3: -200.0} ,iteration: 305 ,episode: 1\n",
      "[[0 0.0]\n",
      " [2 1.0]\n",
      " [2 4.0]\n",
      " [0 0]\n",
      " [2 3.0]\n",
      " [2 2.0]\n",
      " [2 6.0]\n",
      " [2 1.0]\n",
      " [2 2.0]\n",
      " [2 4.0]]\n",
      "1 258 305\n",
      "1 2\n",
      "done: {1: True, 2: True, 3: True} ,total_reward: {1: -100.73999999999998, 2: -200.07999999999998, 3: 109.78} ,iteration: 305 ,episode: 2\n",
      "[[0 0.0]\n",
      " [3 1.0]\n",
      " [3 2.0]\n",
      " [3 2.0]\n",
      " [3 9.0]\n",
      " [3 3.0]\n",
      " [3 4.0]\n",
      " [3 1.0]\n",
      " [3 2.0]\n",
      " [3 5.0]]\n",
      "2 2264 305\n",
      "1 2\n",
      "episode_break_1\n",
      "done: {1: False, 2: True, 3: False} ,total_reward: {1: -95.93999999999998, 2: -200.09, 3: -95.22} ,iteration: 305 ,episode: 3\n",
      "[[1 5.0]\n",
      " [1 2.0]\n",
      " [3 21.0]\n",
      " [1 85.0]\n",
      " [3 1.0]\n",
      " [3 7.0]\n",
      " [3 2.0]\n",
      " [0 0.0]\n",
      " [1 24.0]\n",
      " [1 1.0]]\n",
      "3 3000 305\n",
      "1 2\n",
      "0\n",
      "divi 36 chunk_size 64 batch_shape torch.Size([1, 2239, 256])\n",
      "1\n",
      "divi 24 chunk_size 64 batch_shape torch.Size([1, 1487, 256])\n"
     ]
    }
   ],
   "source": [
    "T.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb01fed5-6286-427e-9b25-adf9565f7fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.env.board.phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec23f1-8da2-4bb1-a65e-110e08890110",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logp_pi_a_2_dir[:, -1,0])\n",
    "print(log_probs_actions_2[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6cc2e51-b059-433b-af26-c641458db21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in T.hero_agents_list[1].model.model.named_parameters():\n",
    "    if not torch.isfinite(param).all():#torch.isinf(param).any():\n",
    "      print(f\"Infinite value found in weight: {name}\")\n",
    "    if param.grad is not None and torch.isinf(param.grad).any():\n",
    "      print(f\"Infinite value found in gradient: {name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd85e4a-675f-4e5f-b413-d0b389261322",
   "metadata": {},
   "source": [
    "- number of steps per episode\n",
    "- number of bad moves\n",
    "- total reward\n",
    "- kind of play\n",
    "- position\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2fa04b-543c-4ce4-ad4f-a39e625eafc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.model_in[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b64c74-d3c6-47d5-84ff-a1765f2f21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.env.last()[0]['observation']\n",
    "\n",
    "\n",
    "T.obs_converter(torch.tensor(T.env.last()[0]['observation']#observation['observation']\n",
    "                                                        ).to(T.device,dtype=torch.float32),\n",
    "                                                        num_classes = T.total_agents+1)[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc4c74-9632-41c0-bb2a-47923a85ef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in T.hero_agents_list[1].model.model.named_parameters():\n",
    "    if torch.isnan(param.grad).any():\n",
    "        print(\"nan gradient found\",name)\n",
    "        raise SystemExit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25444c82-840d-4376-ae1f-8cb96ee8c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize states\n",
    "        #for traj in self.trajectories:\n",
    "        #    traj['observations'] = (traj['observations'].to(dtype=torch.float32) - self.state_mean) / self.state_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240169a9-1453-4971-b7e2-2150b16d4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.state_mean) / self.state_std\n",
    "\n",
    "\n",
    "current_troops_count_mean tensor(5.2496), current_troops_count_std = tensor(1.4733)\n",
    "\n",
    "(x-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df156463-6bb3-4ea6-8178-bb0dff62902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for traj in self.trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313c4ae-8ea6-415c-98db-710d01da5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.cat((states, action_masks * hero_steps,\n",
    "                      current_phase, current_agent,\n",
    "                      current_troops_count[:, :, None]), axis=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd2eb803-7fa2-45ba-8011-8d1493df06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_ = [batch for batch in T.hero_agents_list[1].model.traj_data_loader][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea7a6712-bffa-4d6d-be7d-44603bbb811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "            timesteps,\n",
    "            states,\n",
    "            actions_1,\n",
    "            actions_2,log_probs_actions_2,\n",
    "            returntogo,\n",
    "            returns_to_go_cal,\n",
    "            returntogo_pred,\n",
    "            reward,\n",
    "            traj_mask,\n",
    "            action_masks,\n",
    "            current_agent_acting,\n",
    "            current_agent_simple,\n",
    "            current_agent,\n",
    "            current_phase,\n",
    "            current_troops_count,\n",
    "        ) = dat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26c53f2d-1087-4297-93c8-2e10e243a115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5293, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs_actions_2[0,:,-1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01c1b81b-76c5-4f90-9870-b73619d713cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2580, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5efe372-cd8c-47cc-8019-8765c74db46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7825, 256])\n",
      "torch.Size([1, 797, 256])\n",
      "torch.Size([1, 1753, 256])\n",
      "torch.Size([1, 1021, 256])\n",
      "torch.Size([1, 10000, 256])\n",
      "torch.Size([1, 10000, 256])\n",
      "torch.Size([1, 10000, 256])\n",
      "torch.Size([1, 2464, 256])\n",
      "torch.Size([1, 2413, 256])\n",
      "torch.Size([1, 10000, 256])\n",
      "torch.Size([1, 7611, 256])\n",
      "torch.Size([1, 10000, 256])\n",
      "torch.Size([1, 4520, 256])\n",
      "torch.Size([1, 619, 256])\n",
      "torch.Size([1, 7846, 256])\n",
      "torch.Size([1, 3012, 256])\n",
      "torch.Size([1, 10000, 256])\n",
      "torch.Size([1, 10000, 256])\n",
      "torch.Size([1, 8041, 256])\n",
      "torch.Size([1, 10000, 256])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in T.hero_agents_list[1].model.traj_data_loader:\n",
    "        (\n",
    "            timesteps,\n",
    "            states,\n",
    "            actions_1,\n",
    "            actions_2, lp,\n",
    "            returntogo,\n",
    "            returns_to_go_cal,\n",
    "            returntogo_pred,\n",
    "            reward,\n",
    "            traj_mask,\n",
    "            action_masks,\n",
    "            current_agent_acting,\n",
    "            current_agent_simple,\n",
    "            current_agent,\n",
    "            current_phase,\n",
    "            current_troops_count,\n",
    "        ) = batch\n",
    "        print(actions_2.shape)\n",
    "        if (actions_2[0,-1,:] >1).any():\n",
    "            \n",
    "            print(actions_2[0,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d394cc-2cd4-4ebb-ad33-0e964e727d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ca67d17-008a-4d89-8d3f-5cdca7af658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_ =[batch for batch in T.hero_agents_list[1].model.traj_data_loader][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08aee1e3-014c-4522-870d-8862ef504985",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "            timesteps,\n",
    "            states,\n",
    "            actions_1,\n",
    "            actions_2,\n",
    "            returntogo,\n",
    "            returns_to_go_cal,\n",
    "            returntogo_pred,\n",
    "            reward,\n",
    "            traj_mask,\n",
    "            action_masks,\n",
    "            current_agent_acting,\n",
    "            current_agent_simple,\n",
    "            current_agent,\n",
    "            current_phase,\n",
    "            current_troops_count,\n",
    "        ) =dat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0204385f-8ad3-4242-b7b0-75bd80f175c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5261, 4.5718, 4.6180, 4.6646, 4.7118, 4.7594, 4.8074, 4.8560, 4.9050,\n",
       "        4.9546, 5.0046, 5.0552, 5.1062, 4.1477, 4.1896, 4.2319, 4.2747, 4.3179,\n",
       "        4.3615, 4.4055, 4.4500, 4.4950, 4.5404, 4.5862, 4.6326, 4.6794, 4.7266,\n",
       "        4.7744, 4.8226, 4.8713, 4.9205, 4.9702, 5.0204, 5.0711, 5.1224, 5.1741,\n",
       "        5.2264, 5.2792, 5.3325, 4.3762, 4.4205, 4.4651, 4.5102, 4.5659, 3.6019,\n",
       "        3.6383, 3.6750, 3.7121, 3.7496, 3.7875, 3.8258, 3.8644, 3.9034, 3.9429,\n",
       "        3.9827, 4.0229, 4.0636, 4.1046, 4.1461, 4.1880, 4.2303, 4.2730, 4.3161,\n",
       "        4.3597, 3.3937, 2.4179, 2.4423, 2.4670, 2.4919, 2.5170, 2.5425, 2.5681,\n",
       "        2.5941, 2.6203, 2.6468, 2.6735, 2.7005, 2.7278, 2.7553, 2.7832, 2.8113,\n",
       "        2.8397, 2.8684, 2.8973, 2.9266, 2.9562, 2.9860, 3.0162, 3.0466, 3.0774,\n",
       "        3.1085, 3.1399, 3.1716, 3.2037, 3.2360, 3.2687, 3.3017, 3.3351, 3.3688,\n",
       "        3.4028, 3.4372, 3.4719, 3.5069, 3.5424, 3.5781, 3.6143, 3.6508, 2.6776,\n",
       "        1.6945, 1.7116, 1.7289, 1.7464, 1.7640, 1.7818, 1.7998, 1.8180, 1.8364,\n",
       "        1.8549, 0.8636, 0.8723, 0.8811, 0.9001, 0.9092, 0.9184, 0.9277, 0.9370,\n",
       "        0.9465, 0.9561, 0.9657, 0.9755, 0.9853, 0.9953, 1.0053, 1.0155, 1.0257,\n",
       "        1.0361, 1.0466, 1.0571, 1.0678, 1.0786, 1.0895, 1.1005, 1.1116, 1.1228,\n",
       "        1.1342, 1.1456, 1.1572, 1.1689, 1.1807, 1.1926, 1.2047, 1.2169, 1.2291,\n",
       "        1.2416, 1.2541, 1.2668, 1.2796, 1.2925, 1.3055, 1.3187, 1.3321, 1.3455,\n",
       "        1.3591, 1.3728, 1.3867, 1.4007, 1.4149, 0.4190, 0.4334, 0.4378, 0.4422,\n",
       "        0.4466, 0.4512, 0.4557, 0.4603, 0.4650, 0.4697, 0.4744, 0.4792, 0.4840,\n",
       "        0.4889, 0.4939, 0.4989, 0.5039, 0.5090, 0.5141, 0.5193, 0.5347, 0.5401,\n",
       "        0.5455, 0.5510, 0.5566, 0.5622, 0.5679, 0.5736, 0.5794, 0.5853, 0.5912,\n",
       "        0.5972, 0.6032, 0.6093, 0.6154, 0.6217, 0.6279, 0.6343, 0.6407, 0.6472,\n",
       "        0.6537, 0.6603, 0.6670, 0.6737, 0.6805, 0.6874, 0.6943, 0.7013, 0.7084,\n",
       "        0.7156, 0.7228, 0.7301, 0.7375, 0.7449, 0.7525, 0.7601, 0.7677, 0.7856,\n",
       "        0.7935, 0.8015, 0.8096, 0.8178, 0.8261, 0.8344, 0.8429, 0.8514, 0.8600,\n",
       "        0.8687, 0.8774, 0.8863, 0.8952, 0.9043, 0.9134, 0.9227, 0.9320, 0.9414,\n",
       "        0.9509, 0.9605, 0.9702, 0.9800, 1.0000], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns_to_go_cal[0,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f107eff-2aa5-48e2-b5a3-caf1f24890c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "divi 3 chunk_size 256 batch_shape torch.Size([1, 578, 256])\n",
    "1\n",
    "divi 7 chunk_size 256 batch_shape torch.Size([1, 1558, 256])\n",
    "2\n",
    "divi 8 chunk_size 256 batch_shape torch.Size([1, 1861, 256])\n",
    "3\n",
    "divi 7 chunk_size 256 batch_shape torch.Size([1, 1691, 256])\n",
    "4\n",
    "divi 4 chunk_size 256 batch_shape torch.Size([1, 959, 256])\n",
    "5\n",
    "divi 11 chunk_size 256 batch_shape torch.Size([1, 2583, 256])\n",
    "6\n",
    "divi 10 chunk_size 256 batch_shape torch.Size([1, 2483, 256])\n",
    "7\n",
    "divi 10 chunk_size 256 batch_shape torch.Size([1, 2447, 256])\n",
    "8\n",
    "divi 17 chunk_size 256 batch_shape torch.Size([1, 4308, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ddbc84-e488-4e88-93ce-6941b9b3d94d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "current_troops_count_std,current_troops_count_mean = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in T.hero_agents_list[1].model.traj_data_loader:\n",
    "        (\n",
    "            timesteps,\n",
    "            states,\n",
    "            actions_1,\n",
    "            actions_2,\n",
    "            returntogo,\n",
    "            returns_to_go_cal,\n",
    "            returntogo_pred,\n",
    "            reward,\n",
    "            traj_mask,\n",
    "            action_masks,\n",
    "            current_agent_acting,\n",
    "            current_agent_simple,\n",
    "            current_agent,\n",
    "            current_phase,\n",
    "            current_troops_count,\n",
    "        ) = dat_\n",
    "\n",
    "        current_troops_count_std.append(current_troops_count[:torch.where(action_masks[0,:,-1,:].sum(-1) != 0)[0][-1]+1].std())\n",
    "        current_troops_count_mean.append(current_troops_count[:torch.where(action_masks[0,:,-1,:].sum(-1) != 0)[0][-1]+1].mean())\n",
    "        \n",
    "\n",
    "torch.mean(torch.tensor(current_troops_count_std)),torch.mean(torch.tensor(current_troops_count_mean))\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ed48c-a46e-446d-acc4-5a0ed8fdb535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(\n",
    "    timesteps,\n",
    "    states,\n",
    "    actions_1,\n",
    "    actions_2,\n",
    "    returntogo,\n",
    "    returns_to_go_cal,\n",
    "    returntogo_pred,\n",
    "    reward,\n",
    "    traj_mask,\n",
    "    action_masks,\n",
    "    current_agent_acting,\n",
    "    current_agent_simple,\n",
    "    current_agent,\n",
    "    current_phase,\n",
    "    current_troops_count,\n",
    "    ) = dat_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ae545-7b59-4fd5-bce8-e0685888bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.isfinite(returns_to_go_cal).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77e6ad5f-a910-4110-9904-5354e90084d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2580, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(action_masks[0,:,-1,:].sum(-1) != 0)[0][-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea52ce0e-c1c2-4b1f-80d6-9499dd9b8908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = 'cpu'#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "timesteps = timesteps[0,:2580].to(device)  # B x T\n",
    "states = states[0,:2580].to(device)  # B x T x state_dim\n",
    "actions_1 = actions_1[0,:2580].to(device)  # B x T x act_dim\n",
    "actions_2 = actions_2[0,:2580].to(device) \n",
    "reward = reward[0,:2580].to(device)\n",
    "returns_to_go_cal =   returns_to_go_cal[0,:2580].to(device).unsqueeze(dim=-1)  # B x T x 1\n",
    "returntogo = returntogo[0,:2580].to(device)\n",
    "returntogo_pred = returntogo_pred[0,:2580].to(device)\n",
    "traj_mask = traj_mask[0,:2580].to(device)  # B x T\n",
    "action_masks = action_masks[0,:2580].to(device)\n",
    "current_agent_acting = current_agent_acting[0,:2580].to(device)\n",
    "current_agent_simple = current_agent_simple[0,:2580].to(device)\n",
    "current_agent = current_agent[0,:2580].to(device)\n",
    "current_phase = current_phase[0,:2580].to(device)\n",
    "current_troops_count = current_troops_count[0,:2580].to(device)\n",
    "\n",
    "info = dict({})\n",
    "\n",
    "hero_steps = current_agent_simple == 1#self.hero\n",
    "\n",
    "states = torch.cat((states, action_masks * hero_steps,\n",
    "                      current_phase, current_agent,\n",
    "                      current_troops_count[:, :, None]), axis=2)  # ,torch.ones(len(action_masks))[:,None]*self.hero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dad7c6-c9bb-497f-ab55-c392600ad86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98afca5a-108b-46c2-b1b9-1745d083f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05c289-2f20-49f5-97a9-4021c3da82e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(timesteps.device,\n",
    "states.device,\n",
    "actions_1.device,\n",
    "actions_2.device,\n",
    "returntogo.device,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b95ae-0e26-415b-aebd-ee1f57ddafb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae40e56-2609-4268-9eaa-48c6d33a7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  action_logit_model_1,action_model_2_dir,_) =   self.model.forward(timesteps=timesteps, states=states,\n",
    "#                        actions_1=actions_1,actions_2=actions_2,\n",
    "#                        returns_to_go=returns_to_go, print_=2,return_logit=True)  # print_\n",
    "\n",
    "#                                                 # ,info = info\n",
    "\n",
    "# actions_1_ = actions_1.clone().detach()#, requires_grad=False)\n",
    "\n",
    "# # self.actions_ = actions_\n",
    "# # self.action_preds_model_ = action_preds_model\n",
    "# # actions_[:,:,-1] = action_preds_model\n",
    "\n",
    "\n",
    "\n",
    "# actions_1_[:, -1] = action_logit_model_1[:, -1, :\n",
    "#         ].argmax(axis=1).clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dd9bb29-f001-4ed6-9abf-e0b7d7b6f694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 256, 1]), torch.Size([10, 256]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_go_pred.shape,actions_2[:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba48dc87-6427-4f2d-aad8-cc687500854b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "189253a8-b3fd-4900-b179-b99e8768d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_,actions_go_pred, returntogo_pred_,lp = T.hero_agents_list[1].model.model.forward(\n",
    "                                                        timesteps=timesteps[:10],\n",
    "                                                        states=states[:10],\n",
    "                                                        actions_1=actions_1[:10],\n",
    "                                                        actions_2=actions_2[:10],\n",
    "                                                        returns_to_go=returntogo[:10],\n",
    "                                                            print_=2,#return_log_prob_a2= True\n",
    "                                                                return_og_log_prob_a2 = True\n",
    "                                                        #,info = info\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52adcb8f-7fc8-4cd4-bd12-af1ba83b418a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2009,  0.5237,  0.4434, -0.3261,  0.5088,  0.0081, -0.9799,  0.4514,\n",
       "        -2.7107, -0.8660], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp[:,-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017177e6-12b3-437a-9504-d12cc57202bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(a1_[:,-1,:][hero_steps[:,-1,0]]*action_masks[:,-1,:][hero_steps[:,-1,0]]).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159c832-8cec-4532-8116-6d3bdfe07b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f2a2c2-83c5-4320-8cdb-d9d7ada6693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_go_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d267ee-055d-4224-9e3c-f9947833d918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actions[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1bbf39-2de8-4537-822f-bd3a6bb6eb24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_agent_acting[:,-1,0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce5402-4013-4c2a-8ab6-36b6acadfe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea8d3b-e676-482e-b124-34784bfa7b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.where(actions_go_pred[:,-1,:32].argmax(dim=1) == actions[:,-1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04c451-52be-4670-8ff4-d59c1479a687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actions_go_pred[:,-1,:32].argmax(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f8e9b-3c31-4667-b523-df1f7e78a577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ece91c-d3d4-495a-9d5c-fa0bc86b90b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.DT_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df551102-4826-43eb-a5e1-31257e1de7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.DT_input['return_to_go']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0e98b-4832-4483-be01-e8c959a7dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.requires_grad for i in  [batch for batch in T.hero_agents_list[1].model.traj_data_loader][0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ac9829-1010-44a5-a1c5-99fc47010464",
   "metadata": {},
   "outputs": [],
   "source": [
    "[batch for batch in T.hero_agents_list[1].model.traj_data_loader][0].trajectories[0]['observations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3269f-21c3-41f4-9f31-5b17dda15a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc412c-f067-49d7-b464-5a84e55abdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "[batch for batch in T.hero_agents_list[1].model.traj_data_loader][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b91a2-c770-4b41-b7f5-9168c7855027",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.actions_[:,-1,0] = T.hero_agents_list[1].model.action_preds_model_[:,0,:32].argmax(axis =1)\n",
    "T.hero_agents_list[1].model.actions_[:,-1,1] = T.hero_agents_list[1].model.action_preds_model_[:,0,32]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe37c0a-23d1-4379-9f13-bc0c02774e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.action_preds_model_[:,0,:32].argmax(axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911eb982-f0b0-4e45-bc20-1fbdbf9f420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.action_preds_model_[:,0,32].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed269783-13c4-4318-82d3-63f9a8fa569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps,                #torch.Size([1, 2475, 256])    \n",
    "states,                   #torch.Size([1, 2475, 256, 50]) \n",
    "actions,                  #torch.Size([1, 2475, 256, 2])  \n",
    "returntogo,               #torch.Size([1, 2475, 256, 1])     \n",
    "returns_to_go_cal,        #torch.Size([1, 2475, 256])            \n",
    "returntogo_pred,          #torch.Size([1, 2475, 256, 1])          \n",
    "reward,                   #torch.Size([1, 2475, 256]) \n",
    "traj_mask ,               #torch.Size([1, 2475, 256])     \n",
    "action_masks,             #torch.Size([1, 2475, 256, 32])       \n",
    "current_agent_simple,     #torch.Size([1, 2475, 256, 1])               \n",
    "current_agent,            #torch.Size([1, 2475, 256, 3])        \n",
    "current_phase,            #torch.Size([1, 2475, 256, 2])        \n",
    "current_troops_count      #torch.Size([1, 2475, 256])              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b68c12a-8262-4aee-9d7c-20badee6800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(timesteps,                #torch.Size([1, 2475, 256])    \n",
    "states,                   #torch.Size([1, 2475, 256, 50]) \n",
    "actions,                  #torch.Size([1, 2475, 256, 2])  \n",
    "returntogo,               #torch.Size([1, 2475, 256, 1])     \n",
    "returns_to_go_cal,        #torch.Size([1, 2475, 256])            \n",
    "returntogo_pred,          #torch.Size([1, 2475, 256, 1])          \n",
    "reward,                   #torch.Size([1, 2475, 256]) \n",
    "traj_mask ,               #torch.Size([1, 2475, 256])     \n",
    "action_masks,             #torch.Size([1, 2475, 256, 32])       \n",
    "current_agent_simple,     #torch.Size([1, 2475, 256, 1])               \n",
    "current_agent,            #torch.Size([1, 2475, 256, 3])        \n",
    "current_phase,            #torch.Size([1, 2475, 256, 2])        \n",
    "current_troops_count      #torch.Size([1, 2475, 256])    \n",
    ")= [batch for batch in T.hero_agents_list[1].model.traj_data_loader][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21858ecf-8cc9-405a-92c4-5ae5a978189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RT1 = reward_2[:-1] + gamma*returns_target_2[1:]\n",
    "Q_TD = torch.nn.functional.smooth_l1_loss(return_preds_2[:-1], RT1)\n",
    "Q_MSE = torch.nn.functional.smooth_l1_loss(return_preds_2, returns_to_go_2)\n",
    "\n",
    "\n",
    "reward_2 = reward.squeeze().view(-1)[traj_mask.view(-1,) > 0].to(self.device)\n",
    "return_preds_2 = return_preds.squeeze().view(-1, #act_dim\n",
    "                                            )[traj_mask.view(-1,) > 0].to(self.device)\n",
    "returns_target_2 = returns_target.squeeze().view(-1)[traj_mask.view(-1,) > 0].to(self.device)\n",
    "returns_to_go_2 = returns_to_go.squeeze().view(-1)[traj_mask.view(-1,) > 0].to(self.device)\n",
    "returns_to_go_cal_2 = returns_to_go_cal.squeeze().view(-1)[traj_mask.view(-1,) > 0].to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3069bc-a418-4596-bb5e-b2ebbb4f3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446cd6ca-2956-4573-91e5-ad17ae770cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "returntogo_pred.squeeze().view(-1, #act_dim\n",
    "                                            )[traj_mask.view(-1,) > 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ddbf75-6dde-4d32-a8c0-3664959bfb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_to_go_cal.squeeze().view(-1)[traj_mask.view(-1,) > 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86828a5a-1791-49c0-9f35-ef7737c33bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_to_go_cal[:,:,-1].squeeze().view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872e330-39b2-4f6b-9d2b-5da4d27d413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1beb164-3f68-4b53-85f7-3544a6005b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "hero_steps = (current_agent_simple[0] == 1 )\n",
    "\n",
    "#states_ = torch.concat(\n",
    "    \n",
    "(states[0].shape,\n",
    "         (action_masks[0]*hero_steps).shape,\n",
    "         current_phase[0].shape,\n",
    "         current_agent[0].shape,\n",
    "         #,torch.ones(len(action_masks))[:,None]*self.hero\n",
    "         current_troops_count[0][:,:,None].shape)\n",
    "#        ), axis =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315abaec-5412-46c2-91b0-36bbeac772f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.model.embed_state(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcbdb55-c30d-4688-a3cf-497a0c58f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(timesteps,             #torch.Size([1, 7229, 256])      \n",
    " states,                #torch.Size([1, 7229, 256, 50])   \n",
    " actions,               #torch.Size([1, 7229, 256, 2])    \n",
    " returns_to_go,         #torch.Size([1, 7229, 256, 4])         \n",
    " reward,                #torch.Size([1, 7229, 256, 4])  \n",
    " traj_mask ,            #torch.Size([1, 7229, 256])      \n",
    " action_masks,          #torch.Size([1, 7229, 256, 32])        \n",
    " current_agent_simple,  #torch.Size([1, 7229, 256, 1])                \n",
    " current_agent,         #torch.Size([1, 7229, 256, 3])         \n",
    " current_phase,         #torch.Size([1, 7229, 256, 2])         \n",
    " current_troops_count   #torch.Size([1, 7229, 256, 4])                \n",
    ")= [batch for batch in T.hero_agents_list[1].model.traj_data_loader][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98438ecc-6ef9-4a42-8f62-78c9f1540684",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_to_go[0].to(T.device).unsqueeze(dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2867e151-fec3-4725-a05d-e26dc6802f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_to_go.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d6c58a-c731-478c-ad4d-77c61cfb5b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [batch for batch in T.hero_agents_list[1].model.traj_data_loader][0]:\n",
    "    print(k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c04ace6-e054-4a79-aabf-56aa83859343",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.model(timesteps= self.DT_input['timestep'], states=self.DT_input['state'][None,:],\n",
    "                          actions=self.DT_input['action'],\n",
    "                          returns_to_go=self.DT_input['return_to_go']\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc3665-2b47-40c5-93c9-a2c7fba70869",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.DT_input['timestep'].long().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377063cc-63bb-40c6-9f25-a93623c8317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.model.embed_timestep(\n",
    "T.hero_agents_list[1].model.DT_input['timestep'].long()\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c85c9-d1ed-40a6-9402-35479cba7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "(T.hero_agents_list[1].model.DT_input['timestep'].shape,\n",
    "T.hero_agents_list[1].model.DT_input['state'].shape,\n",
    "T.hero_agents_list[1].model.DT_input['action'].shape,\n",
    "T.hero_agents_list[1].model.DT_input['return_to_go'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f2c21-74ed-4f1d-a2de-e2765223e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.model.embed_state(\n",
    "T.hero_agents_list[1].model.DT_input['state'][None,:].float()\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f4d0f-b6f1-4d14-a044-d63311168e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.model.embed_state(\n",
    "T.hero_agents_list[1].model.DT_input['state'].repeat(3,1,1).float()\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47038127-2e9d-4e2a-8b3f-7984d1dfa920",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.model.embed_action_1(\n",
    "    T.hero_agents_list[1].model.DT_input['action'][:,:,0].long()\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1f936-3a61-4353-88c9-80ceb1016cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.model.embed_action(\n",
    "torch.concat((\n",
    "T.hero_agents_list[1].model.model.embed_action_1(\n",
    "    T.hero_agents_list[1].model.DT_input['action'][:,:,0].long()\n",
    "    ),\n",
    "\n",
    "    T.hero_agents_list[1].model.model.embed_action_2(\n",
    "     T.hero_agents_list[1].model.DT_input['action'][:,:,None][:,:,:,1])), axis =2)).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4878d07-e392-4415-b8b4-316ce4b057c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.returntogo_pred[T.hero_agents_list[1].model.DT_input['timestep'][0,-1]]\n",
    "\n",
    "\n",
    "s,a,R = T.hero_agents_list[1].model.model(\n",
    "\n",
    "\n",
    "\n",
    "    timesteps= T.hero_agents_list[1].model.DT_input['timestep'], \n",
    "        states=T.hero_agents_list[1].model.DT_input['state'],\n",
    "       actions=T.hero_agents_list[1].model.DT_input['action'],\n",
    "returns_to_go=T.hero_agents_list[1].model.DT_input['return_to_go']\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2a752-dd0f-407c-a8b7-a57acfbddea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.hero_agents_list[1].model.act_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73754fe-ec06-4cbe-9957-bee61fd1102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0,0,:32].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d24f65-eaef-45ef-a2ce-bc3bf7c24613",
   "metadata": {},
   "outputs": [],
   "source": [
    "="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bd810-8cc7-4088-9ec8-00f2e9a6924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d020ab8-d2e0-431d-8d7d-f0ed6637db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "R[0,0]#.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa60ccb-7f7a-4ebd-a4db-64e2342f5dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T.hero_agents_list[1].model.model.embed_action(\n",
    "torch.concat((\n",
    "T.hero_agents_list[1].model.model.embed_action_1(\n",
    "    T.hero_agents_list[1].model.DT_input['action'].repeat(3,1,1)[:,:,0].long()\n",
    "    ),\n",
    "\n",
    "    T.hero_agents_list[1].model.model.embed_action_2(\n",
    "     T.hero_agents_list[1].model.DT_input['action'].repeat(3,1,1)[:,:,None][:,:,:,1])), axis =2)).shape\n",
    "\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e73ff-3049-4319-b9da-dc7e3f06cfe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3fb76b-73ef-4c32-a3d5-b9b6ebb14f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(timesteps, states, actions, returns_to_go,reward, traj_mask ,action_masks,\n",
    " current_agent_simple,current_agent,current_phase,current_troops_count) = [i for i in T.traj_data_loader ][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5418a-3d1d-4488-8a50-3b6aba8b9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "(timesteps, states, actions, returns_to_go,reward, traj_mask ,action_masks,\n",
    " current_agent_simple,current_agent,current_phase,current_troops_count) = (\n",
    "     timesteps[0], states[0], actions[0], returns_to_go[0],reward[0], traj_mask[0] ,action_masks[0],\n",
    " current_agent_simple[0],current_agent[0],current_phase[0],current_troops_count[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8ce7f-58ff-451d-be6d-084bbda9c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "hero_steps = (current_agent_simple ==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1160eea4-0178-4118-8a07-7681ec559123",
   "metadata": {},
   "outputs": [],
   "source": [
    "hero_steps = (current_agent_simple ==1)\n",
    "torch.concat((states,\n",
    "                 action_masks*hero_steps,\n",
    "                 \n",
    "                 current_phase,\n",
    "                 current_agent\n",
    "            \n",
    "                #,torch.ones(len(action_masks))[:,None]*self.hero\n",
    "                ,current_troops_count[:,:,1,None]\n",
    "                ), axis =2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c5e2c7-f84a-4499-ae4b-b312ef1d0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict(render_mode = 'rgb_array', default_attack_all  = True,\n",
    "                render_ = True,agent_count  = 3,use_placement_perc=True)\n",
    "env = env_risk(**(env_config | {\"render_mode\" : None,\"verbose\":False, \"agent_count\" :4,\"render_\":False}))\n",
    "\n",
    "\n",
    "total_rewards = {i:0 for i in env.possible_agents}\n",
    "\n",
    "step =0\n",
    "for i__ in range(1):\n",
    "    env.reset()#seed=42)\n",
    "    #print(env_1.infos.keys())\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    \n",
    "    total_rewards = {i:0 for i in env.possible_agents}\n",
    "    \n",
    "    #so there is an issue ...reward =2 sometimes ... doubling?\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        step+=1\n",
    "        e_t = env.terminations\n",
    "        if sum(e_t.values()) <3:\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            \n",
    "            action = env.action_space(agent).sample()\n",
    "            \n",
    "            #if env_1.phase_selection ==1:\n",
    "            #    action = [int(action[0]),action[1]]\n",
    "            part_0 =np.random.choice(np.where(env.board.calculated_action_mask(agent))[0])\n",
    "            #print(part_0)\n",
    "            \n",
    "            #action = [part_0,np.around(action[1],2) #min(action[1],env_1.board.agents[agent].bucket ) \n",
    "            #                  if env_1.phase_selection==0 else  action[1]]\n",
    "    \n",
    "            action = [part_0,np.around(action[1],2)]\n",
    "    \n",
    "    \n",
    "            for i in env.possible_agents:\n",
    "                total_rewards[i]+=env.curr_rewards[i]\n",
    "            \n",
    "            env.step(action)\n",
    "\n",
    "            if step%10000 ==0:\n",
    "                print(step)\n",
    "                print(env.board.territories)\n",
    "                \n",
    "                print('\\naction',action,\n",
    "                      '\\ninfo',info,\n",
    "                      '\\ninfos',env.infos,\n",
    "                      '\\naction_valid',observation['action_mask'][action[0]],\n",
    "                      '\\nagent',agent,\n",
    "                      '\\nselected_agent',env.agent_selection,\n",
    "                      '\\ncurr_agent',env.board.current_agent,\n",
    "                      '\\ncurr phase',env.phase_selection,\n",
    "                      '\\nbad_trail count',env.board.bad_trials,\n",
    "                      '\\nmax_bad trails', env.board.max_bad_trials,\n",
    "                      '\\nreward',reward,\n",
    "                      '\\nreward',env.curr_rewards,\n",
    "                      '\\ntotal_reward',total_rewards,\n",
    "                      '\\nrewards',env.rewards,\n",
    "                      '\\nbuckets', [env.board.agents[i].bucket for i in env.agents]\n",
    "                     )\n",
    "                print('kill_list',env.kill_list,'term',e_t, 'buckets',[i.bucket for i in env.board.agents])\n",
    "                #if sum(env.curr_rewards.values()) <0:\n",
    "                #    input()\n",
    "        else:\n",
    "            print(env.board.territories)\n",
    "            print('done',env.terminations.values(),env.curr_rewards)\n",
    "            break\n",
    "step\n",
    "\n",
    "\n",
    "#need to debug .... so the action is valid... but there is no change\n",
    "#so either the phase is not changing and the agent is not changing... why??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f192e8ae-31f6-444d-a117-f627d55d13c4",
   "metadata": {},
   "source": [
    "# transformer module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045029a-0489-40b9-b249-178db83216ea",
   "metadata": {},
   "source": [
    "## masked attention and block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e656b18e-a997-4e59-8860-4d6f389f481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "#import gym\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import pickle\n",
    "import tqdm\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\"\"\"\n",
    "this extremely minimal GPT model is based on:\n",
    "Misha Laskin's tweet: \n",
    "https://twitter.com/MishaLaskin/status/1481767788775628801?cxt=HHwWgoCzmYD9pZApAAAA\n",
    "\n",
    "and its corresponding notebook:\n",
    "https://colab.research.google.com/drive/1NUBqyboDcGte5qAJKOl8gaJC28V_73Iv?usp=sharing\n",
    "\n",
    "the above colab has a bug while applying masked_fill which is fixed in the\n",
    "following code\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "\n",
    "class MaskedCausalAttention(nn.Module):\n",
    "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.max_T = max_T\n",
    "\n",
    "        self.q_net = layer_init(nn.Linear(h_dim, h_dim), std=0.01)\n",
    "        self.k_net = layer_init(nn.Linear(h_dim, h_dim), std=0.01)\n",
    "        self.v_net = layer_init(nn.Linear(h_dim, h_dim), std=0.01)\n",
    "\n",
    "        self.proj_net = layer_init(nn.Linear(h_dim, h_dim), std=0.01)\n",
    "\n",
    "        self.att_drop = nn.Dropout(drop_p)\n",
    "        self.proj_drop = nn.Dropout(drop_p)\n",
    "\n",
    "        ones = torch.ones((max_T, max_T))\n",
    "        mask = torch.tril(ones).view(1, 1, max_T, max_T)\n",
    "\n",
    "        # register buffer makes sure mask does not get updated\n",
    "        # during backpropagation\n",
    "        self.register_buffer('mask',mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # batch size, seq length, h_dim * n_heads\n",
    "\n",
    "        N, D = self.n_heads, C // self.n_heads # N = num heads, D = attention dim\n",
    "\n",
    "        # rearrange q, k, v as (B, N, T, D)\n",
    "        q = self.q_net(x).view(B, T, N, D).transpose(1,2)\n",
    "        k = self.k_net(x).view(B, T, N, D).transpose(1,2)\n",
    "        v = self.v_net(x).view(B, T, N, D).transpose(1,2)\n",
    "\n",
    "        # weights (B, N, T, T)\n",
    "        weights = q @ k.transpose(2,3) / math.sqrt(D)\n",
    "        # causal mask applied to weights\n",
    "        weights = weights.masked_fill(self.mask[...,:T,:T] == 0, float('-inf'))\n",
    "        # normalize weights, all -inf -> 0 after softmax\n",
    "        normalized_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # attention (B, N, T, D)\n",
    "        attention = self.att_drop(normalized_weights @ v)\n",
    "\n",
    "        # gather heads and project (B, N, T, D) -> (B, T, N*D)\n",
    "        attention = attention.transpose(1, 2).contiguous().view(B,T,N*D)\n",
    "\n",
    "        out = self.proj_drop(self.proj_net(attention))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "        self.attention = MaskedCausalAttention(h_dim, max_T, n_heads, drop_p)\n",
    "        self.mlp = nn.Sequential(\n",
    "                layer_init(nn.Linear(h_dim, 4*h_dim), std=0.01),\n",
    "                nn.GELU(),\n",
    "                layer_init(nn.Linear(4*h_dim, h_dim), std=0.01),\n",
    "                nn.Dropout(drop_p),\n",
    "            )\n",
    "        self.ln1 = nn.LayerNorm(h_dim)\n",
    "        self.ln2 = nn.LayerNorm(h_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention -> LayerNorm -> MLP -> LayerNorm\n",
    "        x = x + self.attention(x) # residual\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.mlp(x) # residual\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834fb1d-611a-4e1a-bac4-26c9bc6b4fd5",
   "metadata": {},
   "source": [
    "## DT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194368b-29aa-4ed9-8255-ed15fff0d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, state_dim, act_dim, n_blocks, h_dim, context_len, \n",
    "                 n_heads, drop_p, max_timestep=4096):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "        ### transformer blocks\n",
    "        input_seq_len = 3 * context_len\n",
    "        blocks = [Block(h_dim, input_seq_len, n_heads, drop_p) for _ in range(n_blocks)]\n",
    "        self.transformer = nn.Sequential(*blocks)\n",
    "\n",
    "        ### projection heads (project to embedding)\n",
    "        self.embed_ln = nn.LayerNorm(h_dim)\n",
    "        self.embed_timestep = nn.Embedding(max_timestep, h_dim)\n",
    "        self.embed_rtg = layer_init(torch.nn.Linear(1, h_dim), std=0.01)\n",
    "        self.embed_state = layer_init(torch.nn.Linear(state_dim, h_dim), std=0.01)\n",
    "        \n",
    "        # # discrete actions - maybe i'll not use it for risk... will look into this later\n",
    "        self.embed_action_1 = torch.nn.Embedding(max_timestep, h_dim) # not act_dim\n",
    "        self.embed_action_2 = layer_init(torch.nn.Linear(1, h_dim), std=0.01)\n",
    "        self.embed_action = torch.nn.Linear(2*h_dim,h_dim)\n",
    "        use_action_tanh = False # False for discrete actions\n",
    "\n",
    "        # continuous actions\n",
    "        #self.embed_action = torch.nn.Linear(act_dim, h_dim)\n",
    "        #use_action_tanh = True # True for continuous actions\n",
    "        \n",
    "        ### prediction heads\n",
    "        #self.predict_rtg = torch.nn.Linear(h_dim, 1)\n",
    "        self.predict_rtg = layer_init(torch.nn.Linear(h_dim, act_dim), std=0.01)\n",
    "        self.predict_state = layer_init(torch.nn.Linear(h_dim, state_dim), std=0.01)\n",
    "        self.predict_action = nn.Sequential(\n",
    "            *([layer_init(nn.Linear(h_dim, act_dim), std=0.01)] + ([nn.Tanh()] if use_action_tanh else []))\n",
    "        )\n",
    "\n",
    "        self.predict_actor_1 = nn.Sequential(\n",
    "                                        *([layer_init(nn.Linear(h_dim, act_dim), std=0.01)] +\n",
    "                                          ([nn.Tanh()] if use_action_tanh else []))\n",
    "                                                        )\n",
    "        self.predict_actor_2 = nn.Sequential(layer_init(nn.Linear(h_dim, 2), std=0.01), \n",
    "                                     nn.Softmax(dim=1),)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, timesteps, states, actions, returns_to_go):\n",
    "\n",
    "        B, T, _ = states.shape\n",
    "\n",
    "        time_embeddings = self.embed_timestep(timesteps)\n",
    "\n",
    "        # time embeddings are treated similar to positional embeddings\n",
    "        state_embeddings = self.embed_state(states) + time_embeddings\n",
    "        #print(actions.shape)\n",
    "        #print(self.embed_action(actions).squeeze().shape)\n",
    "        #print(time_embeddings.shape)\n",
    "        \n",
    "        action_embeddings = self.embed_action(\n",
    "                                            torch.concat(\n",
    "                                                (self.embed_action_1(actions[:,0].squeeze()),\n",
    "                                                 self.embed_action_2(actions[:,1].squeeze())),axis =1)\n",
    "                                )+ time_embeddings\n",
    "        #embed_action_1\n",
    "        #embed_action_2\n",
    "        #embed_action\n",
    "\n",
    "        \n",
    "        #action_embeddings = self.embed_action(actions.squeeze())+ time_embeddings\n",
    "        returns_embeddings = self.embed_rtg(returns_to_go.float()) + time_embeddings\n",
    "\n",
    "        # stack rtg, states and actions and reshape sequence as\n",
    "        # (r1, s1, a1, r2, s2, a2 ...)\n",
    "        h = torch.stack(\n",
    "            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n",
    "        ).permute(0, 2, 1, 3).reshape(B, 3 * T, self.h_dim)\n",
    "\n",
    "        h = self.embed_ln(h)\n",
    "        \n",
    "        # transformer and prediction\n",
    "        h = self.transformer(h)\n",
    "\n",
    "        # get h reshaped such that its size = (B x 3 x T x h_dim) and\n",
    "        # h[:, 0, t] is conditioned on r_0, s_0, a_0 ... r_t\n",
    "        # h[:, 1, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t\n",
    "        # h[:, 2, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t, a_t\n",
    "        h = h.reshape(B, T, 3, self.h_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # get predictions\n",
    "        return_preds = self.predict_rtg(h[:,2])     # predict next rtg given r, s, a\n",
    "        state_preds = self.predict_state(h[:,2])    # predict next state given r, s, a\n",
    "        #action_preds = self.predict_action(h[:,1])  # predict action given r, s\n",
    "\n",
    "\n",
    "        action_preds_1 = self.predict_actor_1(h[:,1])\n",
    "        action_preds_2 = self.predict_actor_2(h[:,1])\n",
    "        action_preds = torch.concat((action_preds_1,action_preds_2),axis =1)\n",
    "    \n",
    "        return state_preds, action_preds, return_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b748c-d4fc-4b5e-99c1-dfd2a06bb743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd7dee2d-254c-4dc0-9ad8-b4d752e4484c",
   "metadata": {},
   "source": [
    "# custom sampler for the hero agent \n",
    "(but will not be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d78f9-fbf6-418c-9d78-f884163b6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import  Sampler\n",
    "\n",
    "class YourSampler(Sampler[list[int]]):\n",
    "    def __init__(self, mask,data_len):\n",
    "        self.mask = mask[:,None]\n",
    "        self.indices = np.arange(data_len)\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        return (self.indices[i] for i in self.mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mask)\n",
    "\n",
    "\n",
    "sampler1 = YourSampler(torch.tensor([2,3]),4)\n",
    "\n",
    "trainloader_sampler1 = torch.utils.data.DataLoader([([1],[2]),([5],[2]),([6],[3]),([7],[4])], batch_size=4,\n",
    "                                          sampler = sampler1, shuffle=False)\n",
    "[batch for batch in trainloader_sampler1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f2cad-ba8f-49f6-8aa6-001d66789d41",
   "metadata": {},
   "source": [
    "# old dataset and definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bed4f2-9df3-4866-a58e-14b12d51bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class D4RLTrajectoryDataset(Dataset):\n",
    "    def __init__(self, trajectories, context_len, rtg_scale,gamma=0.99,min_len = 10**6):\n",
    "\n",
    "        self.context_len = context_len\n",
    "        self.trajectories = trajectories\n",
    "\n",
    "        #print(len(self.trajectories))\n",
    "        # load dataset\n",
    "        #with open(dataset_path, 'rb') as f:\n",
    "        #    self.trajectories = pickle.load(f)\n",
    "\n",
    "        \n",
    "        # calculate min len of traj, state mean and variance\n",
    "        # and returns_to_go for all traj\n",
    "\n",
    "        \n",
    "        \n",
    "        states = []\n",
    "        for traj in self.trajectories:\n",
    "            traj_len = traj['observations'].shape[0]\n",
    "            min_len = min(min_len, traj_len)\n",
    "            states.append(traj['observations'])\n",
    "            # calculate returns to go and rescale them\n",
    "            \n",
    "            traj['returns_to_go'] = discount_cumsum(traj['rewards'], gamma) / rtg_scale\n",
    "            \n",
    "        print(min_len)\n",
    "        \n",
    "        # used for input normalization\n",
    "        states = torch.concatenate(states, axis=0).to(dtype = torch.float32)\n",
    "        self.state_mean, self.state_std = torch.mean(states, axis=0), torch.std(states, axis=0) + 1e-6\n",
    "\n",
    "        # normalize states\n",
    "        #for traj in self.trajectories:\n",
    "        #    traj['observations'] = (traj['observations'].to(dtype=torch.float32) - self.state_mean) / self.state_std\n",
    "\n",
    "\n",
    "    def get_state_stats(self):\n",
    "        return self.state_mean, self.state_std\n",
    "\n",
    "    def __len__(self):\n",
    "        #print(len(self.trajectories))\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj = self.trajectories[idx]\n",
    "        traj_len = traj['observations'].shape[0]\n",
    "\n",
    "        if traj_len >= self.context_len:\n",
    "            # sample random index to slice trajectory\n",
    "            si = random.randint(0, traj_len - self.context_len)\n",
    "\n",
    "            states = (traj['observations'][si : si + self.context_len])\n",
    "            actions = (traj['actions'][si : si + self.context_len])\n",
    "            reward =  (traj['rewards'][si : si + self.context_len])\n",
    "            returns_to_go = (traj['returns_to_go'][si : si + self.context_len])\n",
    "            \n",
    "            action_masks = (traj['action_masks'][si : si + self.context_len])\n",
    "            current_agent_simple = (traj['current_agent_simple'][si : si + self.context_len])\n",
    "            current_agent = (traj['current_agent'][si : si + self.context_len])\n",
    "            current_phase = (traj['current_phase'][si : si + self.context_len])\n",
    "            current_troops_count = (traj['current_troops_count'][si : si + self.context_len])\n",
    "\n",
    "            \n",
    "            timesteps = torch.arange(start=si, end=si+self.context_len, step=1)\n",
    "\n",
    "            # all ones since no padding\n",
    "            traj_mask = torch.ones(self.context_len, dtype=torch.long)\n",
    "\n",
    "        else:\n",
    "            padding_len = self.context_len - traj_len\n",
    "\n",
    "            # padding with zeros\n",
    "            states = (traj['observations'])\n",
    "            states = torch.cat([states,\n",
    "                                torch.zeros(([padding_len] + list(states.shape[1:])),\n",
    "                                dtype=states.dtype)], \n",
    "                               dim=0)\n",
    "            \n",
    "            actions = (traj['actions'])\n",
    "            actions = torch.cat([actions,\n",
    "                                torch.zeros(([padding_len] + list(actions.shape[1:])),\n",
    "                                dtype=actions.dtype)], \n",
    "                               dim=0)\n",
    "            reward = (traj['rewards'])\n",
    "            reward = torch.cat([reward,\n",
    "                                torch.zeros(([padding_len] + list(reward.shape[1:])),\n",
    "                                dtype=reward.dtype\n",
    "                                           )], \n",
    "                               dim=0)\n",
    "            returns_to_go = (traj['returns_to_go'])\n",
    "            returns_to_go = torch.cat([returns_to_go,\n",
    "                                torch.zeros(([padding_len] + list(returns_to_go.shape[1:])),\n",
    "                                dtype=returns_to_go.dtype\n",
    "                                           )], \n",
    "                               dim=0)\n",
    "\n",
    "            action_masks = (traj['action_masks'][si : si + self.context_len])\n",
    "            current_agent_simple = (traj['current_agent_simple'][si : si + self.context_len])\n",
    "            current_agent = (traj['current_agent'][si : si + self.context_len])\n",
    "            current_phase = (traj['current_phase'][si : si + self.context_len])\n",
    "            current_troops_count = (traj['current_troops_count'][si : si + self.context_len])\n",
    "\n",
    "            action_masks = torch.cat([action_masks,\n",
    "                                torch.zeros(([padding_len] + list(action_masks.shape[1:])),\n",
    "                                dtype=action_masks.dtype\n",
    "                                           )], \n",
    "                               dim=0)\n",
    "            current_agent_simple = torch.cat([current_agent_simple,\n",
    "                                torch.zeros(([padding_len] + list(current_agent_simple.shape[1:])),\n",
    "                                dtype=current_agent_simple.dtype\n",
    "                                           )], \n",
    "                               dim=0)\n",
    "            current_agent = torch.cat([current_agent,\n",
    "                                torch.zeros(([padding_len] + list(current_agent.shape[1:])),\n",
    "                                dtype=current_agent.dtype\n",
    "                                           )], \n",
    "                               dim=0)\n",
    "\n",
    "            current_phase = torch.cat([current_phase,\n",
    "                                torch.zeros(([padding_len] + list(current_phase.shape[1:])),\n",
    "                                dtype=current_phase.dtype\n",
    "                                           )], \n",
    "                               dim=0)\n",
    "\n",
    "            current_troops_count = torch.cat([current_troops_count,\n",
    "                                torch.zeros(([padding_len] + list(current_troops_count.shape[1:])),\n",
    "                                dtype=current_troops_count.dtype\n",
    "                                           )], \n",
    "                               dim=0)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            timesteps = torch.arange(start=0, end=self.context_len, step=1)\n",
    "\n",
    "            traj_mask = torch.cat([torch.ones(traj_len, dtype=torch.long), \n",
    "                                   torch.zeros(padding_len, dtype=torch.long)], \n",
    "                                  dim=0)\n",
    "            \n",
    "        return  timesteps, states, actions, returns_to_go,reward, traj_mask ,action_masks,current_agent_simple,current_agent,current_phase,current_troops_count\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL_project)",
   "language": "python",
   "name": "dl_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
